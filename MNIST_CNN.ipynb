{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KdyKBNPeaKZ8"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPlr50YdcHBeDUiZWZXmOb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "463037dec77c46c285f19a6eee1b6a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f55fa9636ab24828be15a1c575596654",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b54603c8f46440ca8dfe69879deef624",
              "IPY_MODEL_79ccbbc29f7b477b8ae00f4326122c2e"
            ]
          }
        },
        "f55fa9636ab24828be15a1c575596654": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b54603c8f46440ca8dfe69879deef624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c0dd8bf1ac1e412a904da5fdb228b628",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fee1066b13894be1ab3c2f5c42826874"
          }
        },
        "79ccbbc29f7b477b8ae00f4326122c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_767cc8acbea94528bfae58c6a784eba9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:02&lt;00:00,  1.48 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ef786da28724331a9e570c194c1a56b"
          }
        },
        "c0dd8bf1ac1e412a904da5fdb228b628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fee1066b13894be1ab3c2f5c42826874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "767cc8acbea94528bfae58c6a784eba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ef786da28724331a9e570c194c1a56b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rufimelo99/MNIST/blob/main/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwf7L7rExxyg"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZnYjFYGo71_"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sklearn.decomposition\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYesKuzdyes-"
      },
      "source": [
        "***MNIST Dataset***\n",
        "\n",
        "70000 Examples:\n",
        "\n",
        "**Splited**:\n",
        "\n",
        "* 60000 for training;\n",
        "\n",
        "* 10000 for tests;\n",
        "\n",
        "\n",
        "**10 Classes**: \n",
        "\n",
        "* 0, ..., 9\n",
        "\n",
        "**Features**\n",
        "\n",
        "* (28,28,1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cfdH1KbpdUX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230,
          "referenced_widgets": [
            "463037dec77c46c285f19a6eee1b6a24",
            "f55fa9636ab24828be15a1c575596654",
            "b54603c8f46440ca8dfe69879deef624",
            "79ccbbc29f7b477b8ae00f4326122c2e",
            "c0dd8bf1ac1e412a904da5fdb228b628",
            "fee1066b13894be1ab3c2f5c42826874",
            "767cc8acbea94528bfae58c6a784eba9",
            "9ef786da28724331a9e570c194c1a56b"
          ]
        },
        "outputId": "6189c65b-8055-43da-a06d-f323069ce262"
      },
      "source": [
        "mnist_data, mnist_info = tfds.load('mnist', with_info=True)\n",
        "#print(mnist_data)\n",
        "#print(mnist_info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "463037dec77c46c285f19a6eee1b6a24",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ie2tgR662H"
      },
      "source": [
        "It is needed to convert into numpy arrays and normalize the images. Each pixel range from 0 to 255.0, so:\n",
        "\n",
        "`pixels = pixels/255.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXC3Eng6p4iE"
      },
      "source": [
        "mnist_x = np.asarray([instance['image'] for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "mnist_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['train'])])\n",
        "mnist_x= mnist_x/255.0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et1LoPNqf4Xw"
      },
      "source": [
        "mnist_test_x = np.asarray([instance['image'] for instance in tfds.as_numpy(mnist_data['test'])])\n",
        "mnist_test_y = np.asarray([instance['label'] for instance in tfds.as_numpy(mnist_data['test'])])\n",
        "mnist_test_x= mnist_test_x/255.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX-DhgZiKsNL"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdyKBNPeaKZ8"
      },
      "source": [
        "##Number of Kernels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FwSIbpGDaQT5",
        "outputId": "52867880-6ace-47af-d657-79a9a44c173c"
      },
      "source": [
        "kernelSizes = [1,2,3,4,5,6]\n",
        "acc_values = [None] * len(kernelSizes)\n",
        "for kernels in range(len(kernelSizes)):\n",
        "  mnist_conv_model = tf.keras.Sequential(name='mnist_cnn')\n",
        "  mnist_conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "  mnist_conv_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=kernelSizes[kernels], activation='relu', padding='same', name='convolution'))\n",
        "  mnist_conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "  mnist_conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "  mnist_conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "  mnist_conv_model.summary()\n",
        "\n",
        "  earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "  checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "  mnist_conv_model_train = mnist_conv_model.fit(mnist_x, mnist_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)\n",
        "\n",
        "  mnist_conv_model.load_weights('mnist_conv_best.h5')\n",
        "  loss, acc = mnist_conv_model.evaluate(mnist_x, mnist_y)\n",
        "  print('Accuracy for the training set: {}'.format(acc))\n",
        "  loss, acc = mnist_conv_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "  print('Accuracy for the testing set: {}'.format(acc))\n",
        "  fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "  acc_values[kernels]=acc\n",
        "\n",
        "  loss_ax.set_title('Loss')\n",
        "  loss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\n",
        "  loss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "  acc_ax.set_title('Accuracy')\n",
        "  acc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\n",
        "  acc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "plt.plot(acc_values) # plotting by columns\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution (Conv2D)         (None, 28, 28, 16)        32        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                125450    \n",
            "=================================================================\n",
            "Total params: 125,482\n",
            "Trainable params: 125,482\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "188/188 [==============================] - 29s 31ms/step - loss: 1.6907 - accuracy: 0.6895 - val_loss: 0.9967 - val_accuracy: 0.8163\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.81625, saving model to mnist_conv_best.h5\n",
            "Epoch 2/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.7130 - accuracy: 0.8412 - val_loss: 0.5612 - val_accuracy: 0.8608\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.81625 to 0.86075, saving model to mnist_conv_best.h5\n",
            "Epoch 3/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.4985 - accuracy: 0.8687 - val_loss: 0.4587 - val_accuracy: 0.8767\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.86075 to 0.87667, saving model to mnist_conv_best.h5\n",
            "Epoch 4/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.4297 - accuracy: 0.8819 - val_loss: 0.4129 - val_accuracy: 0.8864\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.87667 to 0.88642, saving model to mnist_conv_best.h5\n",
            "Epoch 5/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3936 - accuracy: 0.8904 - val_loss: 0.3856 - val_accuracy: 0.8917\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.88642 to 0.89167, saving model to mnist_conv_best.h5\n",
            "Epoch 6/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3705 - accuracy: 0.8962 - val_loss: 0.3686 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.89167 to 0.89650, saving model to mnist_conv_best.h5\n",
            "Epoch 7/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3547 - accuracy: 0.8996 - val_loss: 0.3554 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.89650 to 0.90067, saving model to mnist_conv_best.h5\n",
            "Epoch 8/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3429 - accuracy: 0.9029 - val_loss: 0.3461 - val_accuracy: 0.9018\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90067 to 0.90183, saving model to mnist_conv_best.h5\n",
            "Epoch 9/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3338 - accuracy: 0.9046 - val_loss: 0.3392 - val_accuracy: 0.9042\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90183 to 0.90417, saving model to mnist_conv_best.h5\n",
            "Epoch 10/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3264 - accuracy: 0.9067 - val_loss: 0.3321 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90417 to 0.90592, saving model to mnist_conv_best.h5\n",
            "Epoch 11/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3204 - accuracy: 0.9089 - val_loss: 0.3280 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.90592 to 0.90650, saving model to mnist_conv_best.h5\n",
            "Epoch 12/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3151 - accuracy: 0.9100 - val_loss: 0.3246 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.90650 to 0.90842, saving model to mnist_conv_best.h5\n",
            "Epoch 13/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3109 - accuracy: 0.9116 - val_loss: 0.3216 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.90842 to 0.90933, saving model to mnist_conv_best.h5\n",
            "Epoch 14/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3070 - accuracy: 0.9128 - val_loss: 0.3176 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90933 to 0.91050, saving model to mnist_conv_best.h5\n",
            "Epoch 15/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.3036 - accuracy: 0.9135 - val_loss: 0.3166 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.91050 to 0.91083, saving model to mnist_conv_best.h5\n",
            "Epoch 16/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.3007 - accuracy: 0.9144 - val_loss: 0.3123 - val_accuracy: 0.9123\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.91083 to 0.91233, saving model to mnist_conv_best.h5\n",
            "Epoch 17/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2980 - accuracy: 0.9154 - val_loss: 0.3105 - val_accuracy: 0.9128\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.91233 to 0.91275, saving model to mnist_conv_best.h5\n",
            "Epoch 18/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2955 - accuracy: 0.9161 - val_loss: 0.3090 - val_accuracy: 0.9142\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.91275 to 0.91417, saving model to mnist_conv_best.h5\n",
            "Epoch 19/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2933 - accuracy: 0.9170 - val_loss: 0.3078 - val_accuracy: 0.9154\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.91417 to 0.91542, saving model to mnist_conv_best.h5\n",
            "Epoch 20/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2913 - accuracy: 0.9172 - val_loss: 0.3059 - val_accuracy: 0.9152\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91542\n",
            "Epoch 21/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2894 - accuracy: 0.9178 - val_loss: 0.3046 - val_accuracy: 0.9153\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91542\n",
            "Epoch 22/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2876 - accuracy: 0.9186 - val_loss: 0.3041 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.91542 to 0.91600, saving model to mnist_conv_best.h5\n",
            "Epoch 23/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2861 - accuracy: 0.9196 - val_loss: 0.3035 - val_accuracy: 0.9158\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91600\n",
            "Epoch 24/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2847 - accuracy: 0.9200 - val_loss: 0.3026 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.91600 to 0.91700, saving model to mnist_conv_best.h5\n",
            "Epoch 25/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2831 - accuracy: 0.9203 - val_loss: 0.3007 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.91700 to 0.91800, saving model to mnist_conv_best.h5\n",
            "Epoch 26/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2818 - accuracy: 0.9207 - val_loss: 0.2999 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91800\n",
            "Epoch 27/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2805 - accuracy: 0.9210 - val_loss: 0.2995 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91800\n",
            "Epoch 28/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2794 - accuracy: 0.9214 - val_loss: 0.2992 - val_accuracy: 0.9176\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91800\n",
            "Epoch 29/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2783 - accuracy: 0.9220 - val_loss: 0.2981 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91800\n",
            "Epoch 30/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2772 - accuracy: 0.9226 - val_loss: 0.2977 - val_accuracy: 0.9171\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91800\n",
            "Epoch 31/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2762 - accuracy: 0.9228 - val_loss: 0.2966 - val_accuracy: 0.9188\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.91800 to 0.91883, saving model to mnist_conv_best.h5\n",
            "Epoch 32/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2752 - accuracy: 0.9228 - val_loss: 0.2962 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91883\n",
            "Epoch 33/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2743 - accuracy: 0.9234 - val_loss: 0.2962 - val_accuracy: 0.9182\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91883\n",
            "Epoch 34/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2735 - accuracy: 0.9232 - val_loss: 0.2959 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91883\n",
            "Epoch 35/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2725 - accuracy: 0.9238 - val_loss: 0.2952 - val_accuracy: 0.9178\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91883\n",
            "Epoch 36/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2717 - accuracy: 0.9238 - val_loss: 0.2949 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91883\n",
            "Epoch 37/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2709 - accuracy: 0.9244 - val_loss: 0.2947 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91883\n",
            "Epoch 38/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2702 - accuracy: 0.9245 - val_loss: 0.2938 - val_accuracy: 0.9179\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91883\n",
            "Epoch 39/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2692 - accuracy: 0.9251 - val_loss: 0.2960 - val_accuracy: 0.9186\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91883\n",
            "Epoch 40/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2687 - accuracy: 0.9247 - val_loss: 0.2936 - val_accuracy: 0.9176\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91883\n",
            "Epoch 41/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2679 - accuracy: 0.9257 - val_loss: 0.2928 - val_accuracy: 0.9199\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.91883 to 0.91992, saving model to mnist_conv_best.h5\n",
            "Epoch 42/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2673 - accuracy: 0.9255 - val_loss: 0.2922 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.91992 to 0.92017, saving model to mnist_conv_best.h5\n",
            "Epoch 43/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2667 - accuracy: 0.9262 - val_loss: 0.2929 - val_accuracy: 0.9181\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.92017\n",
            "Epoch 44/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2662 - accuracy: 0.9259 - val_loss: 0.2918 - val_accuracy: 0.9193\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.92017\n",
            "Epoch 45/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2654 - accuracy: 0.9262 - val_loss: 0.2929 - val_accuracy: 0.9181\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.92017\n",
            "Epoch 46/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2650 - accuracy: 0.9262 - val_loss: 0.2930 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.92017\n",
            "Epoch 47/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2644 - accuracy: 0.9261 - val_loss: 0.2909 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.92017 to 0.92025, saving model to mnist_conv_best.h5\n",
            "Epoch 48/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2638 - accuracy: 0.9262 - val_loss: 0.2917 - val_accuracy: 0.9206\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.92025 to 0.92058, saving model to mnist_conv_best.h5\n",
            "Epoch 49/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2634 - accuracy: 0.9268 - val_loss: 0.2900 - val_accuracy: 0.9205\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.92058\n",
            "Epoch 50/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2628 - accuracy: 0.9270 - val_loss: 0.2903 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.92058\n",
            "Epoch 51/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2622 - accuracy: 0.9273 - val_loss: 0.2899 - val_accuracy: 0.9198\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.92058\n",
            "Epoch 52/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2618 - accuracy: 0.9271 - val_loss: 0.2899 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.92058\n",
            "Epoch 53/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2612 - accuracy: 0.9271 - val_loss: 0.2901 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.92058\n",
            "Epoch 54/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2607 - accuracy: 0.9276 - val_loss: 0.2892 - val_accuracy: 0.9190\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.92058\n",
            "Epoch 55/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2602 - accuracy: 0.9275 - val_loss: 0.2910 - val_accuracy: 0.9196\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.92058\n",
            "Epoch 56/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2596 - accuracy: 0.9275 - val_loss: 0.2891 - val_accuracy: 0.9188\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.92058\n",
            "Epoch 57/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2595 - accuracy: 0.9284 - val_loss: 0.2891 - val_accuracy: 0.9198\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.92058\n",
            "Epoch 58/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2590 - accuracy: 0.9280 - val_loss: 0.2886 - val_accuracy: 0.9208\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.92058 to 0.92075, saving model to mnist_conv_best.h5\n",
            "Epoch 59/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2585 - accuracy: 0.9283 - val_loss: 0.2898 - val_accuracy: 0.9193\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.92075\n",
            "Epoch 60/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2581 - accuracy: 0.9287 - val_loss: 0.2897 - val_accuracy: 0.9198\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.92075\n",
            "Epoch 61/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2577 - accuracy: 0.9290 - val_loss: 0.2886 - val_accuracy: 0.9197\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.92075\n",
            "Epoch 62/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2573 - accuracy: 0.9286 - val_loss: 0.2882 - val_accuracy: 0.9207\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.92075\n",
            "Epoch 63/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2568 - accuracy: 0.9286 - val_loss: 0.2879 - val_accuracy: 0.9205\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.92075\n",
            "Epoch 64/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2564 - accuracy: 0.9283 - val_loss: 0.2894 - val_accuracy: 0.9198\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.92075\n",
            "Epoch 65/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2561 - accuracy: 0.9287 - val_loss: 0.2879 - val_accuracy: 0.9207\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.92075\n",
            "Epoch 66/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2556 - accuracy: 0.9295 - val_loss: 0.2893 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00066: val_accuracy improved from 0.92075 to 0.92125, saving model to mnist_conv_best.h5\n",
            "Epoch 67/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2554 - accuracy: 0.9293 - val_loss: 0.2877 - val_accuracy: 0.9199\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.92125\n",
            "Epoch 68/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2552 - accuracy: 0.9290 - val_loss: 0.2879 - val_accuracy: 0.9201\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.92125\n",
            "Epoch 69/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2548 - accuracy: 0.9290 - val_loss: 0.2881 - val_accuracy: 0.9208\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.92125\n",
            "Epoch 70/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2544 - accuracy: 0.9292 - val_loss: 0.2882 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.92125\n",
            "Epoch 71/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2540 - accuracy: 0.9291 - val_loss: 0.2873 - val_accuracy: 0.9213\n",
            "\n",
            "Epoch 00071: val_accuracy improved from 0.92125 to 0.92133, saving model to mnist_conv_best.h5\n",
            "Epoch 72/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2537 - accuracy: 0.9296 - val_loss: 0.2880 - val_accuracy: 0.9199\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.92133\n",
            "Epoch 73/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2535 - accuracy: 0.9296 - val_loss: 0.2871 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.92133\n",
            "Epoch 74/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2530 - accuracy: 0.9295 - val_loss: 0.2888 - val_accuracy: 0.9201\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.92133\n",
            "Epoch 75/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2528 - accuracy: 0.9298 - val_loss: 0.2891 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.92133\n",
            "Epoch 76/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2525 - accuracy: 0.9295 - val_loss: 0.2886 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.92133\n",
            "Epoch 77/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2521 - accuracy: 0.9297 - val_loss: 0.2874 - val_accuracy: 0.9215\n",
            "\n",
            "Epoch 00077: val_accuracy improved from 0.92133 to 0.92150, saving model to mnist_conv_best.h5\n",
            "Epoch 78/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2517 - accuracy: 0.9303 - val_loss: 0.2881 - val_accuracy: 0.9218\n",
            "\n",
            "Epoch 00078: val_accuracy improved from 0.92150 to 0.92183, saving model to mnist_conv_best.h5\n",
            "Epoch 79/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2515 - accuracy: 0.9304 - val_loss: 0.2883 - val_accuracy: 0.9220\n",
            "\n",
            "Epoch 00079: val_accuracy improved from 0.92183 to 0.92200, saving model to mnist_conv_best.h5\n",
            "Epoch 80/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2512 - accuracy: 0.9301 - val_loss: 0.2874 - val_accuracy: 0.9206\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.92200\n",
            "Epoch 81/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2509 - accuracy: 0.9305 - val_loss: 0.2873 - val_accuracy: 0.9204\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.92200\n",
            "Epoch 82/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2507 - accuracy: 0.9298 - val_loss: 0.2872 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.92200\n",
            "Epoch 83/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2505 - accuracy: 0.9303 - val_loss: 0.2877 - val_accuracy: 0.9218\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.92200\n",
            "Epoch 84/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2502 - accuracy: 0.9302 - val_loss: 0.2870 - val_accuracy: 0.9217\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.92200\n",
            "Epoch 85/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2497 - accuracy: 0.9304 - val_loss: 0.2867 - val_accuracy: 0.9219\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.92200\n",
            "Epoch 86/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2494 - accuracy: 0.9310 - val_loss: 0.2875 - val_accuracy: 0.9205\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.92200\n",
            "Epoch 87/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2493 - accuracy: 0.9307 - val_loss: 0.2883 - val_accuracy: 0.9207\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.92200\n",
            "Epoch 88/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2491 - accuracy: 0.9310 - val_loss: 0.2883 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.92200\n",
            "Epoch 89/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2486 - accuracy: 0.9308 - val_loss: 0.2880 - val_accuracy: 0.9221\n",
            "\n",
            "Epoch 00089: val_accuracy improved from 0.92200 to 0.92208, saving model to mnist_conv_best.h5\n",
            "Epoch 90/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2486 - accuracy: 0.9314 - val_loss: 0.2876 - val_accuracy: 0.9217\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.92208\n",
            "Epoch 91/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2482 - accuracy: 0.9311 - val_loss: 0.2869 - val_accuracy: 0.9213\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.92208\n",
            "Epoch 92/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2480 - accuracy: 0.9310 - val_loss: 0.2872 - val_accuracy: 0.9216\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.92208\n",
            "Epoch 93/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2479 - accuracy: 0.9309 - val_loss: 0.2875 - val_accuracy: 0.9222\n",
            "\n",
            "Epoch 00093: val_accuracy improved from 0.92208 to 0.92217, saving model to mnist_conv_best.h5\n",
            "Epoch 94/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2475 - accuracy: 0.9311 - val_loss: 0.2879 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.92217\n",
            "Epoch 95/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2471 - accuracy: 0.9315 - val_loss: 0.2869 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.92217\n",
            "Epoch 96/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2471 - accuracy: 0.9313 - val_loss: 0.2867 - val_accuracy: 0.9218\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.92217\n",
            "Epoch 97/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2468 - accuracy: 0.9314 - val_loss: 0.2897 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.92217\n",
            "Epoch 98/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2468 - accuracy: 0.9316 - val_loss: 0.2870 - val_accuracy: 0.9218\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.92217\n",
            "Epoch 99/10000\n",
            "188/188 [==============================] - 6s 30ms/step - loss: 0.2463 - accuracy: 0.9310 - val_loss: 0.2883 - val_accuracy: 0.9205\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.92217\n",
            "Epoch 100/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2462 - accuracy: 0.9314 - val_loss: 0.2873 - val_accuracy: 0.9210\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.92217\n",
            "Epoch 101/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2459 - accuracy: 0.9314 - val_loss: 0.2877 - val_accuracy: 0.9210\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.92217\n",
            "Epoch 102/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2458 - accuracy: 0.9317 - val_loss: 0.2887 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.92217\n",
            "Epoch 103/10000\n",
            "188/188 [==============================] - 6s 31ms/step - loss: 0.2458 - accuracy: 0.9313 - val_loss: 0.2878 - val_accuracy: 0.9216\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.92217\n",
            "Epoch 00103: early stopping\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2546 - accuracy: 0.9301\n",
            "Accuracy for the training set: 0.9301000237464905\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2689 - accuracy: 0.9254\n",
            "Accuracy for the testing set: 0.9254000186920166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution (Conv2D)         (None, 28, 28, 16)        80        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                125450    \n",
            "=================================================================\n",
            "Total params: 125,530\n",
            "Trainable params: 125,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "188/188 [==============================] - 11s 55ms/step - loss: 1.1566 - accuracy: 0.7581 - val_loss: 0.5683 - val_accuracy: 0.8563\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.85633, saving model to mnist_conv_best.h5\n",
            "Epoch 2/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.4746 - accuracy: 0.8724 - val_loss: 0.4223 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.85633 to 0.88425, saving model to mnist_conv_best.h5\n",
            "Epoch 3/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.3942 - accuracy: 0.8897 - val_loss: 0.3784 - val_accuracy: 0.8946\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88425 to 0.89458, saving model to mnist_conv_best.h5\n",
            "Epoch 4/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.3618 - accuracy: 0.8972 - val_loss: 0.3557 - val_accuracy: 0.9013\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89458 to 0.90133, saving model to mnist_conv_best.h5\n",
            "Epoch 5/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.3423 - accuracy: 0.9028 - val_loss: 0.3426 - val_accuracy: 0.9038\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90133 to 0.90375, saving model to mnist_conv_best.h5\n",
            "Epoch 6/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.3290 - accuracy: 0.9064 - val_loss: 0.3304 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90375 to 0.90825, saving model to mnist_conv_best.h5\n",
            "Epoch 7/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.3188 - accuracy: 0.9094 - val_loss: 0.3247 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90825\n",
            "Epoch 8/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.3112 - accuracy: 0.9117 - val_loss: 0.3181 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90825 to 0.91017, saving model to mnist_conv_best.h5\n",
            "Epoch 9/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.3051 - accuracy: 0.9135 - val_loss: 0.3134 - val_accuracy: 0.9131\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.91017 to 0.91308, saving model to mnist_conv_best.h5\n",
            "Epoch 10/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.3000 - accuracy: 0.9150 - val_loss: 0.3090 - val_accuracy: 0.9139\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.91308 to 0.91392, saving model to mnist_conv_best.h5\n",
            "Epoch 11/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.2962 - accuracy: 0.9159 - val_loss: 0.3075 - val_accuracy: 0.9145\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.91392 to 0.91450, saving model to mnist_conv_best.h5\n",
            "Epoch 12/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.2928 - accuracy: 0.9168 - val_loss: 0.3040 - val_accuracy: 0.9151\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.91450 to 0.91508, saving model to mnist_conv_best.h5\n",
            "Epoch 13/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.2899 - accuracy: 0.9175 - val_loss: 0.3036 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.91508 to 0.91633, saving model to mnist_conv_best.h5\n",
            "Epoch 14/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.2874 - accuracy: 0.9185 - val_loss: 0.2994 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.91633 to 0.91692, saving model to mnist_conv_best.h5\n",
            "Epoch 15/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.2849 - accuracy: 0.9198 - val_loss: 0.2981 - val_accuracy: 0.9178\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.91692 to 0.91783, saving model to mnist_conv_best.h5\n",
            "Epoch 16/10000\n",
            "188/188 [==============================] - 10s 54ms/step - loss: 0.2829 - accuracy: 0.9194 - val_loss: 0.3007 - val_accuracy: 0.9181\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.91783 to 0.91808, saving model to mnist_conv_best.h5\n",
            "Epoch 17/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.2811 - accuracy: 0.9208 - val_loss: 0.2963 - val_accuracy: 0.9189\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.91808 to 0.91892, saving model to mnist_conv_best.h5\n",
            "Epoch 18/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2793 - accuracy: 0.9216 - val_loss: 0.2951 - val_accuracy: 0.9191\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.91892 to 0.91908, saving model to mnist_conv_best.h5\n",
            "Epoch 19/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.2776 - accuracy: 0.9215 - val_loss: 0.2947 - val_accuracy: 0.9188\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91908\n",
            "Epoch 20/10000\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.2761 - accuracy: 0.9222 - val_loss: 0.2919 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91908\n",
            "Epoch 21/10000\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.2749 - accuracy: 0.9230 - val_loss: 0.2921 - val_accuracy: 0.9196\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.91908 to 0.91958, saving model to mnist_conv_best.h5\n",
            "Epoch 22/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.2734 - accuracy: 0.9230 - val_loss: 0.2888 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.91958 to 0.92033, saving model to mnist_conv_best.h5\n",
            "Epoch 23/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2722 - accuracy: 0.9234 - val_loss: 0.2909 - val_accuracy: 0.9201\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.92033\n",
            "Epoch 24/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2711 - accuracy: 0.9244 - val_loss: 0.2885 - val_accuracy: 0.9208\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.92033 to 0.92075, saving model to mnist_conv_best.h5\n",
            "Epoch 25/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.2699 - accuracy: 0.9240 - val_loss: 0.2923 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.92075\n",
            "Epoch 26/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2687 - accuracy: 0.9246 - val_loss: 0.2887 - val_accuracy: 0.9197\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.92075\n",
            "Epoch 27/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.2676 - accuracy: 0.9250 - val_loss: 0.2895 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.92075 to 0.92108, saving model to mnist_conv_best.h5\n",
            "Epoch 28/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2668 - accuracy: 0.9250 - val_loss: 0.2859 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.92108\n",
            "Epoch 29/10000\n",
            "188/188 [==============================] - 10s 55ms/step - loss: 0.2659 - accuracy: 0.9256 - val_loss: 0.2866 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.92108\n",
            "Epoch 30/10000\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.2649 - accuracy: 0.9254 - val_loss: 0.2845 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.92108\n",
            "Epoch 31/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2638 - accuracy: 0.9261 - val_loss: 0.2838 - val_accuracy: 0.9207\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.92108\n",
            "Epoch 32/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2628 - accuracy: 0.9259 - val_loss: 0.2829 - val_accuracy: 0.9214\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.92108 to 0.92142, saving model to mnist_conv_best.h5\n",
            "Epoch 33/10000\n",
            "188/188 [==============================] - 10s 56ms/step - loss: 0.2619 - accuracy: 0.9267 - val_loss: 0.2846 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.92142\n",
            "Epoch 34/10000\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.2612 - accuracy: 0.9269 - val_loss: 0.2820 - val_accuracy: 0.9227\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.92142 to 0.92267, saving model to mnist_conv_best.h5\n",
            "Epoch 35/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2601 - accuracy: 0.9278 - val_loss: 0.2813 - val_accuracy: 0.9218\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.92267\n",
            "Epoch 36/10000\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.2592 - accuracy: 0.9277 - val_loss: 0.2804 - val_accuracy: 0.9227\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.92267 to 0.92275, saving model to mnist_conv_best.h5\n",
            "Epoch 37/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2583 - accuracy: 0.9278 - val_loss: 0.2829 - val_accuracy: 0.9215\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.92275\n",
            "Epoch 38/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2576 - accuracy: 0.9284 - val_loss: 0.2800 - val_accuracy: 0.9219\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.92275\n",
            "Epoch 39/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2567 - accuracy: 0.9283 - val_loss: 0.2796 - val_accuracy: 0.9227\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.92275\n",
            "Epoch 40/10000\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.2559 - accuracy: 0.9283 - val_loss: 0.2787 - val_accuracy: 0.9227\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.92275\n",
            "Epoch 41/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2549 - accuracy: 0.9292 - val_loss: 0.2766 - val_accuracy: 0.9235\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.92275 to 0.92350, saving model to mnist_conv_best.h5\n",
            "Epoch 42/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2540 - accuracy: 0.9290 - val_loss: 0.2770 - val_accuracy: 0.9228\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.92350\n",
            "Epoch 43/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2533 - accuracy: 0.9299 - val_loss: 0.2761 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.92350 to 0.92358, saving model to mnist_conv_best.h5\n",
            "Epoch 44/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2526 - accuracy: 0.9292 - val_loss: 0.2782 - val_accuracy: 0.9244\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.92358 to 0.92442, saving model to mnist_conv_best.h5\n",
            "Epoch 45/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2517 - accuracy: 0.9305 - val_loss: 0.2772 - val_accuracy: 0.9235\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.92442\n",
            "Epoch 46/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2507 - accuracy: 0.9299 - val_loss: 0.2741 - val_accuracy: 0.9244\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.92442\n",
            "Epoch 47/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2499 - accuracy: 0.9305 - val_loss: 0.2746 - val_accuracy: 0.9241\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.92442\n",
            "Epoch 48/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2494 - accuracy: 0.9302 - val_loss: 0.2726 - val_accuracy: 0.9252\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.92442 to 0.92517, saving model to mnist_conv_best.h5\n",
            "Epoch 49/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2482 - accuracy: 0.9307 - val_loss: 0.2757 - val_accuracy: 0.9250\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.92517\n",
            "Epoch 50/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2474 - accuracy: 0.9313 - val_loss: 0.2751 - val_accuracy: 0.9245\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.92517\n",
            "Epoch 51/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2466 - accuracy: 0.9317 - val_loss: 0.2711 - val_accuracy: 0.9250\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.92517\n",
            "Epoch 52/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2456 - accuracy: 0.9312 - val_loss: 0.2738 - val_accuracy: 0.9240\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.92517\n",
            "Epoch 53/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2447 - accuracy: 0.9320 - val_loss: 0.2740 - val_accuracy: 0.9232\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.92517\n",
            "Epoch 54/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2439 - accuracy: 0.9325 - val_loss: 0.2708 - val_accuracy: 0.9245\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.92517\n",
            "Epoch 55/10000\n",
            "188/188 [==============================] - 11s 56ms/step - loss: 0.2430 - accuracy: 0.9324 - val_loss: 0.2709 - val_accuracy: 0.9263\n",
            "\n",
            "Epoch 00055: val_accuracy improved from 0.92517 to 0.92633, saving model to mnist_conv_best.h5\n",
            "Epoch 56/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2423 - accuracy: 0.9327 - val_loss: 0.2708 - val_accuracy: 0.9247\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.92633\n",
            "Epoch 57/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2412 - accuracy: 0.9329 - val_loss: 0.2701 - val_accuracy: 0.9253\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.92633\n",
            "Epoch 58/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2402 - accuracy: 0.9329 - val_loss: 0.2685 - val_accuracy: 0.9268\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.92633 to 0.92675, saving model to mnist_conv_best.h5\n",
            "Epoch 59/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2392 - accuracy: 0.9343 - val_loss: 0.2672 - val_accuracy: 0.9287\n",
            "\n",
            "Epoch 00059: val_accuracy improved from 0.92675 to 0.92875, saving model to mnist_conv_best.h5\n",
            "Epoch 60/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2382 - accuracy: 0.9334 - val_loss: 0.2664 - val_accuracy: 0.9263\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.92875\n",
            "Epoch 61/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2373 - accuracy: 0.9339 - val_loss: 0.2660 - val_accuracy: 0.9276\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.92875\n",
            "Epoch 62/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2367 - accuracy: 0.9345 - val_loss: 0.2647 - val_accuracy: 0.9272\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.92875\n",
            "Epoch 63/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2354 - accuracy: 0.9350 - val_loss: 0.2634 - val_accuracy: 0.9279\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.92875\n",
            "Epoch 64/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2344 - accuracy: 0.9349 - val_loss: 0.2648 - val_accuracy: 0.9283\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.92875\n",
            "Epoch 65/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2335 - accuracy: 0.9353 - val_loss: 0.2625 - val_accuracy: 0.9292\n",
            "\n",
            "Epoch 00065: val_accuracy improved from 0.92875 to 0.92917, saving model to mnist_conv_best.h5\n",
            "Epoch 66/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2321 - accuracy: 0.9359 - val_loss: 0.2597 - val_accuracy: 0.9303\n",
            "\n",
            "Epoch 00066: val_accuracy improved from 0.92917 to 0.93033, saving model to mnist_conv_best.h5\n",
            "Epoch 67/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2312 - accuracy: 0.9354 - val_loss: 0.2603 - val_accuracy: 0.9292\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.93033\n",
            "Epoch 68/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2299 - accuracy: 0.9360 - val_loss: 0.2607 - val_accuracy: 0.9290\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.93033\n",
            "Epoch 69/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2294 - accuracy: 0.9365 - val_loss: 0.2584 - val_accuracy: 0.9287\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.93033\n",
            "Epoch 70/10000\n",
            "188/188 [==============================] - 11s 57ms/step - loss: 0.2280 - accuracy: 0.9371 - val_loss: 0.2571 - val_accuracy: 0.9303\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.93033\n",
            "Epoch 71/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2268 - accuracy: 0.9374 - val_loss: 0.2582 - val_accuracy: 0.9302\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.93033\n",
            "Epoch 72/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2259 - accuracy: 0.9379 - val_loss: 0.2536 - val_accuracy: 0.9314\n",
            "\n",
            "Epoch 00072: val_accuracy improved from 0.93033 to 0.93142, saving model to mnist_conv_best.h5\n",
            "Epoch 73/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2246 - accuracy: 0.9381 - val_loss: 0.2571 - val_accuracy: 0.9306\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.93142\n",
            "Epoch 74/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2239 - accuracy: 0.9380 - val_loss: 0.2575 - val_accuracy: 0.9292\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.93142\n",
            "Epoch 75/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2222 - accuracy: 0.9392 - val_loss: 0.2521 - val_accuracy: 0.9308\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.93142\n",
            "Epoch 76/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2214 - accuracy: 0.9388 - val_loss: 0.2511 - val_accuracy: 0.9315\n",
            "\n",
            "Epoch 00076: val_accuracy improved from 0.93142 to 0.93150, saving model to mnist_conv_best.h5\n",
            "Epoch 77/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2200 - accuracy: 0.9389 - val_loss: 0.2504 - val_accuracy: 0.9330\n",
            "\n",
            "Epoch 00077: val_accuracy improved from 0.93150 to 0.93300, saving model to mnist_conv_best.h5\n",
            "Epoch 78/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2184 - accuracy: 0.9397 - val_loss: 0.2537 - val_accuracy: 0.9295\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.93300\n",
            "Epoch 79/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2175 - accuracy: 0.9392 - val_loss: 0.2497 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.93300\n",
            "Epoch 80/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2161 - accuracy: 0.9408 - val_loss: 0.2474 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00080: val_accuracy improved from 0.93300 to 0.93342, saving model to mnist_conv_best.h5\n",
            "Epoch 81/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2149 - accuracy: 0.9404 - val_loss: 0.2463 - val_accuracy: 0.9331\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.93342\n",
            "Epoch 82/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2137 - accuracy: 0.9417 - val_loss: 0.2452 - val_accuracy: 0.9346\n",
            "\n",
            "Epoch 00082: val_accuracy improved from 0.93342 to 0.93458, saving model to mnist_conv_best.h5\n",
            "Epoch 83/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2124 - accuracy: 0.9414 - val_loss: 0.2432 - val_accuracy: 0.9339\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.93458\n",
            "Epoch 84/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2109 - accuracy: 0.9417 - val_loss: 0.2422 - val_accuracy: 0.9346\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.93458\n",
            "Epoch 85/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2096 - accuracy: 0.9427 - val_loss: 0.2399 - val_accuracy: 0.9354\n",
            "\n",
            "Epoch 00085: val_accuracy improved from 0.93458 to 0.93542, saving model to mnist_conv_best.h5\n",
            "Epoch 86/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.2081 - accuracy: 0.9429 - val_loss: 0.2413 - val_accuracy: 0.9357\n",
            "\n",
            "Epoch 00086: val_accuracy improved from 0.93542 to 0.93567, saving model to mnist_conv_best.h5\n",
            "Epoch 87/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.2068 - accuracy: 0.9427 - val_loss: 0.2386 - val_accuracy: 0.9358\n",
            "\n",
            "Epoch 00087: val_accuracy improved from 0.93567 to 0.93583, saving model to mnist_conv_best.h5\n",
            "Epoch 88/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.2056 - accuracy: 0.9434 - val_loss: 0.2373 - val_accuracy: 0.9354\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.93583\n",
            "Epoch 89/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2043 - accuracy: 0.9440 - val_loss: 0.2363 - val_accuracy: 0.9368\n",
            "\n",
            "Epoch 00089: val_accuracy improved from 0.93583 to 0.93683, saving model to mnist_conv_best.h5\n",
            "Epoch 90/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.2030 - accuracy: 0.9448 - val_loss: 0.2361 - val_accuracy: 0.9354\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.93683\n",
            "Epoch 91/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.2014 - accuracy: 0.9448 - val_loss: 0.2345 - val_accuracy: 0.9359\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.93683\n",
            "Epoch 92/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.2001 - accuracy: 0.9453 - val_loss: 0.2362 - val_accuracy: 0.9353\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.93683\n",
            "Epoch 93/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1988 - accuracy: 0.9461 - val_loss: 0.2324 - val_accuracy: 0.9374\n",
            "\n",
            "Epoch 00093: val_accuracy improved from 0.93683 to 0.93742, saving model to mnist_conv_best.h5\n",
            "Epoch 94/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1974 - accuracy: 0.9461 - val_loss: 0.2287 - val_accuracy: 0.9389\n",
            "\n",
            "Epoch 00094: val_accuracy improved from 0.93742 to 0.93892, saving model to mnist_conv_best.h5\n",
            "Epoch 95/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1957 - accuracy: 0.9466 - val_loss: 0.2295 - val_accuracy: 0.9373\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.93892\n",
            "Epoch 96/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1947 - accuracy: 0.9471 - val_loss: 0.2291 - val_accuracy: 0.9378\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.93892\n",
            "Epoch 97/10000\n",
            "188/188 [==============================] - 11s 58ms/step - loss: 0.1931 - accuracy: 0.9471 - val_loss: 0.2263 - val_accuracy: 0.9392\n",
            "\n",
            "Epoch 00097: val_accuracy improved from 0.93892 to 0.93925, saving model to mnist_conv_best.h5\n",
            "Epoch 98/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1915 - accuracy: 0.9482 - val_loss: 0.2246 - val_accuracy: 0.9406\n",
            "\n",
            "Epoch 00098: val_accuracy improved from 0.93925 to 0.94058, saving model to mnist_conv_best.h5\n",
            "Epoch 99/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1900 - accuracy: 0.9487 - val_loss: 0.2234 - val_accuracy: 0.9399\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.94058\n",
            "Epoch 100/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1889 - accuracy: 0.9484 - val_loss: 0.2232 - val_accuracy: 0.9411\n",
            "\n",
            "Epoch 00100: val_accuracy improved from 0.94058 to 0.94108, saving model to mnist_conv_best.h5\n",
            "Epoch 101/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1872 - accuracy: 0.9489 - val_loss: 0.2224 - val_accuracy: 0.9409\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.94108\n",
            "Epoch 102/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1857 - accuracy: 0.9496 - val_loss: 0.2207 - val_accuracy: 0.9408\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.94108\n",
            "Epoch 103/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1842 - accuracy: 0.9506 - val_loss: 0.2193 - val_accuracy: 0.9413\n",
            "\n",
            "Epoch 00103: val_accuracy improved from 0.94108 to 0.94125, saving model to mnist_conv_best.h5\n",
            "Epoch 104/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1832 - accuracy: 0.9496 - val_loss: 0.2174 - val_accuracy: 0.9413\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.94125\n",
            "Epoch 105/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1810 - accuracy: 0.9509 - val_loss: 0.2166 - val_accuracy: 0.9427\n",
            "\n",
            "Epoch 00105: val_accuracy improved from 0.94125 to 0.94275, saving model to mnist_conv_best.h5\n",
            "Epoch 106/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1799 - accuracy: 0.9513 - val_loss: 0.2152 - val_accuracy: 0.9421\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.94275\n",
            "Epoch 107/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1784 - accuracy: 0.9512 - val_loss: 0.2148 - val_accuracy: 0.9420\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.94275\n",
            "Epoch 108/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1770 - accuracy: 0.9520 - val_loss: 0.2120 - val_accuracy: 0.9433\n",
            "\n",
            "Epoch 00108: val_accuracy improved from 0.94275 to 0.94325, saving model to mnist_conv_best.h5\n",
            "Epoch 109/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1759 - accuracy: 0.9521 - val_loss: 0.2111 - val_accuracy: 0.9433\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.94325\n",
            "Epoch 110/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1742 - accuracy: 0.9528 - val_loss: 0.2094 - val_accuracy: 0.9428\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.94325\n",
            "Epoch 111/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1729 - accuracy: 0.9532 - val_loss: 0.2084 - val_accuracy: 0.9437\n",
            "\n",
            "Epoch 00111: val_accuracy improved from 0.94325 to 0.94367, saving model to mnist_conv_best.h5\n",
            "Epoch 112/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1714 - accuracy: 0.9538 - val_loss: 0.2067 - val_accuracy: 0.9434\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.94367\n",
            "Epoch 113/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1699 - accuracy: 0.9537 - val_loss: 0.2041 - val_accuracy: 0.9448\n",
            "\n",
            "Epoch 00113: val_accuracy improved from 0.94367 to 0.94475, saving model to mnist_conv_best.h5\n",
            "Epoch 114/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1682 - accuracy: 0.9543 - val_loss: 0.2053 - val_accuracy: 0.9442\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.94475\n",
            "Epoch 115/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1672 - accuracy: 0.9551 - val_loss: 0.2013 - val_accuracy: 0.9457\n",
            "\n",
            "Epoch 00115: val_accuracy improved from 0.94475 to 0.94575, saving model to mnist_conv_best.h5\n",
            "Epoch 116/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1656 - accuracy: 0.9555 - val_loss: 0.1999 - val_accuracy: 0.9465\n",
            "\n",
            "Epoch 00116: val_accuracy improved from 0.94575 to 0.94650, saving model to mnist_conv_best.h5\n",
            "Epoch 117/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1642 - accuracy: 0.9556 - val_loss: 0.2006 - val_accuracy: 0.9473\n",
            "\n",
            "Epoch 00117: val_accuracy improved from 0.94650 to 0.94725, saving model to mnist_conv_best.h5\n",
            "Epoch 118/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1628 - accuracy: 0.9563 - val_loss: 0.1978 - val_accuracy: 0.9473\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.94725\n",
            "Epoch 119/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1613 - accuracy: 0.9563 - val_loss: 0.1972 - val_accuracy: 0.9463\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.94725\n",
            "Epoch 120/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1600 - accuracy: 0.9565 - val_loss: 0.1943 - val_accuracy: 0.9473\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.94725\n",
            "Epoch 121/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1586 - accuracy: 0.9573 - val_loss: 0.1951 - val_accuracy: 0.9482\n",
            "\n",
            "Epoch 00121: val_accuracy improved from 0.94725 to 0.94817, saving model to mnist_conv_best.h5\n",
            "Epoch 122/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1573 - accuracy: 0.9581 - val_loss: 0.1922 - val_accuracy: 0.9486\n",
            "\n",
            "Epoch 00122: val_accuracy improved from 0.94817 to 0.94858, saving model to mnist_conv_best.h5\n",
            "Epoch 123/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1557 - accuracy: 0.9578 - val_loss: 0.1923 - val_accuracy: 0.9481\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.94858\n",
            "Epoch 124/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1546 - accuracy: 0.9585 - val_loss: 0.1903 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00124: val_accuracy improved from 0.94858 to 0.94925, saving model to mnist_conv_best.h5\n",
            "Epoch 125/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1532 - accuracy: 0.9591 - val_loss: 0.1883 - val_accuracy: 0.9489\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.94925\n",
            "Epoch 126/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1517 - accuracy: 0.9594 - val_loss: 0.1873 - val_accuracy: 0.9500\n",
            "\n",
            "Epoch 00126: val_accuracy improved from 0.94925 to 0.95000, saving model to mnist_conv_best.h5\n",
            "Epoch 127/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1506 - accuracy: 0.9593 - val_loss: 0.1907 - val_accuracy: 0.9497\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.95000\n",
            "Epoch 128/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1492 - accuracy: 0.9599 - val_loss: 0.1857 - val_accuracy: 0.9502\n",
            "\n",
            "Epoch 00128: val_accuracy improved from 0.95000 to 0.95017, saving model to mnist_conv_best.h5\n",
            "Epoch 129/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1479 - accuracy: 0.9600 - val_loss: 0.1833 - val_accuracy: 0.9508\n",
            "\n",
            "Epoch 00129: val_accuracy improved from 0.95017 to 0.95083, saving model to mnist_conv_best.h5\n",
            "Epoch 130/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1466 - accuracy: 0.9604 - val_loss: 0.1826 - val_accuracy: 0.9513\n",
            "\n",
            "Epoch 00130: val_accuracy improved from 0.95083 to 0.95125, saving model to mnist_conv_best.h5\n",
            "Epoch 131/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1454 - accuracy: 0.9613 - val_loss: 0.1812 - val_accuracy: 0.9510\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.95125\n",
            "Epoch 132/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1441 - accuracy: 0.9615 - val_loss: 0.1805 - val_accuracy: 0.9513\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.95125\n",
            "Epoch 133/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1430 - accuracy: 0.9620 - val_loss: 0.1781 - val_accuracy: 0.9529\n",
            "\n",
            "Epoch 00133: val_accuracy improved from 0.95125 to 0.95292, saving model to mnist_conv_best.h5\n",
            "Epoch 134/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1416 - accuracy: 0.9620 - val_loss: 0.1797 - val_accuracy: 0.9514\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.95292\n",
            "Epoch 135/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1407 - accuracy: 0.9628 - val_loss: 0.1765 - val_accuracy: 0.9523\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.95292\n",
            "Epoch 136/10000\n",
            "188/188 [==============================] - 11s 59ms/step - loss: 0.1392 - accuracy: 0.9632 - val_loss: 0.1745 - val_accuracy: 0.9532\n",
            "\n",
            "Epoch 00136: val_accuracy improved from 0.95292 to 0.95325, saving model to mnist_conv_best.h5\n",
            "Epoch 137/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1380 - accuracy: 0.9630 - val_loss: 0.1736 - val_accuracy: 0.9532\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.95325\n",
            "Epoch 138/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1367 - accuracy: 0.9639 - val_loss: 0.1732 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.95325\n",
            "Epoch 139/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1356 - accuracy: 0.9644 - val_loss: 0.1730 - val_accuracy: 0.9532\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.95325\n",
            "Epoch 140/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1343 - accuracy: 0.9641 - val_loss: 0.1727 - val_accuracy: 0.9536\n",
            "\n",
            "Epoch 00140: val_accuracy improved from 0.95325 to 0.95358, saving model to mnist_conv_best.h5\n",
            "Epoch 141/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1333 - accuracy: 0.9648 - val_loss: 0.1700 - val_accuracy: 0.9542\n",
            "\n",
            "Epoch 00141: val_accuracy improved from 0.95358 to 0.95425, saving model to mnist_conv_best.h5\n",
            "Epoch 142/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1321 - accuracy: 0.9647 - val_loss: 0.1705 - val_accuracy: 0.9536\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.95425\n",
            "Epoch 143/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1309 - accuracy: 0.9653 - val_loss: 0.1688 - val_accuracy: 0.9542\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.95425\n",
            "Epoch 144/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1298 - accuracy: 0.9654 - val_loss: 0.1669 - val_accuracy: 0.9557\n",
            "\n",
            "Epoch 00144: val_accuracy improved from 0.95425 to 0.95567, saving model to mnist_conv_best.h5\n",
            "Epoch 145/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1288 - accuracy: 0.9659 - val_loss: 0.1672 - val_accuracy: 0.9548\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.95567\n",
            "Epoch 146/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1276 - accuracy: 0.9666 - val_loss: 0.1648 - val_accuracy: 0.9559\n",
            "\n",
            "Epoch 00146: val_accuracy improved from 0.95567 to 0.95592, saving model to mnist_conv_best.h5\n",
            "Epoch 147/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1264 - accuracy: 0.9664 - val_loss: 0.1629 - val_accuracy: 0.9573\n",
            "\n",
            "Epoch 00147: val_accuracy improved from 0.95592 to 0.95733, saving model to mnist_conv_best.h5\n",
            "Epoch 148/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1256 - accuracy: 0.9672 - val_loss: 0.1626 - val_accuracy: 0.9557\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.95733\n",
            "Epoch 149/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1245 - accuracy: 0.9672 - val_loss: 0.1627 - val_accuracy: 0.9565\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.95733\n",
            "Epoch 150/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1236 - accuracy: 0.9675 - val_loss: 0.1600 - val_accuracy: 0.9572\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.95733\n",
            "Epoch 151/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1222 - accuracy: 0.9679 - val_loss: 0.1587 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00151: val_accuracy improved from 0.95733 to 0.95833, saving model to mnist_conv_best.h5\n",
            "Epoch 152/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1214 - accuracy: 0.9679 - val_loss: 0.1586 - val_accuracy: 0.9579\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.95833\n",
            "Epoch 153/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1203 - accuracy: 0.9683 - val_loss: 0.1588 - val_accuracy: 0.9571\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.95833\n",
            "Epoch 154/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1194 - accuracy: 0.9682 - val_loss: 0.1575 - val_accuracy: 0.9571\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.95833\n",
            "Epoch 155/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1185 - accuracy: 0.9690 - val_loss: 0.1557 - val_accuracy: 0.9584\n",
            "\n",
            "Epoch 00155: val_accuracy improved from 0.95833 to 0.95842, saving model to mnist_conv_best.h5\n",
            "Epoch 156/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1177 - accuracy: 0.9688 - val_loss: 0.1543 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00156: val_accuracy improved from 0.95842 to 0.95858, saving model to mnist_conv_best.h5\n",
            "Epoch 157/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1165 - accuracy: 0.9694 - val_loss: 0.1540 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.95858\n",
            "Epoch 158/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1156 - accuracy: 0.9695 - val_loss: 0.1545 - val_accuracy: 0.9585\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.95858\n",
            "Epoch 159/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1148 - accuracy: 0.9698 - val_loss: 0.1523 - val_accuracy: 0.9597\n",
            "\n",
            "Epoch 00159: val_accuracy improved from 0.95858 to 0.95967, saving model to mnist_conv_best.h5\n",
            "Epoch 160/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1137 - accuracy: 0.9706 - val_loss: 0.1529 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.95967\n",
            "Epoch 161/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1130 - accuracy: 0.9704 - val_loss: 0.1516 - val_accuracy: 0.9590\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.95967\n",
            "Epoch 162/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1119 - accuracy: 0.9709 - val_loss: 0.1500 - val_accuracy: 0.9597\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.95967\n",
            "Epoch 163/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1112 - accuracy: 0.9708 - val_loss: 0.1491 - val_accuracy: 0.9607\n",
            "\n",
            "Epoch 00163: val_accuracy improved from 0.95967 to 0.96075, saving model to mnist_conv_best.h5\n",
            "Epoch 164/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1102 - accuracy: 0.9714 - val_loss: 0.1482 - val_accuracy: 0.9604\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.96075\n",
            "Epoch 165/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1094 - accuracy: 0.9711 - val_loss: 0.1471 - val_accuracy: 0.9611\n",
            "\n",
            "Epoch 00165: val_accuracy improved from 0.96075 to 0.96108, saving model to mnist_conv_best.h5\n",
            "Epoch 166/10000\n",
            "188/188 [==============================] - 11s 60ms/step - loss: 0.1085 - accuracy: 0.9712 - val_loss: 0.1471 - val_accuracy: 0.9611\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.96108\n",
            "Epoch 167/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1077 - accuracy: 0.9719 - val_loss: 0.1467 - val_accuracy: 0.9611\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.96108\n",
            "Epoch 168/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1070 - accuracy: 0.9721 - val_loss: 0.1457 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00168: val_accuracy improved from 0.96108 to 0.96175, saving model to mnist_conv_best.h5\n",
            "Epoch 169/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1061 - accuracy: 0.9722 - val_loss: 0.1437 - val_accuracy: 0.9624\n",
            "\n",
            "Epoch 00169: val_accuracy improved from 0.96175 to 0.96242, saving model to mnist_conv_best.h5\n",
            "Epoch 170/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1053 - accuracy: 0.9727 - val_loss: 0.1434 - val_accuracy: 0.9620\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.96242\n",
            "Epoch 171/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1043 - accuracy: 0.9729 - val_loss: 0.1443 - val_accuracy: 0.9615\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.96242\n",
            "Epoch 172/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1037 - accuracy: 0.9729 - val_loss: 0.1418 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.96242\n",
            "Epoch 173/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1030 - accuracy: 0.9731 - val_loss: 0.1423 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.96242\n",
            "Epoch 174/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1021 - accuracy: 0.9731 - val_loss: 0.1412 - val_accuracy: 0.9622\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.96242\n",
            "Epoch 175/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1013 - accuracy: 0.9737 - val_loss: 0.1406 - val_accuracy: 0.9620\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.96242\n",
            "Epoch 176/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.1005 - accuracy: 0.9738 - val_loss: 0.1406 - val_accuracy: 0.9627\n",
            "\n",
            "Epoch 00176: val_accuracy improved from 0.96242 to 0.96267, saving model to mnist_conv_best.h5\n",
            "Epoch 177/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0999 - accuracy: 0.9742 - val_loss: 0.1395 - val_accuracy: 0.9637\n",
            "\n",
            "Epoch 00177: val_accuracy improved from 0.96267 to 0.96367, saving model to mnist_conv_best.h5\n",
            "Epoch 178/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0993 - accuracy: 0.9744 - val_loss: 0.1383 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.96367\n",
            "Epoch 179/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0985 - accuracy: 0.9742 - val_loss: 0.1376 - val_accuracy: 0.9637\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.96367\n",
            "Epoch 180/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0979 - accuracy: 0.9745 - val_loss: 0.1370 - val_accuracy: 0.9632\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.96367\n",
            "Epoch 181/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0972 - accuracy: 0.9750 - val_loss: 0.1366 - val_accuracy: 0.9639\n",
            "\n",
            "Epoch 00181: val_accuracy improved from 0.96367 to 0.96392, saving model to mnist_conv_best.h5\n",
            "Epoch 182/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0963 - accuracy: 0.9752 - val_loss: 0.1371 - val_accuracy: 0.9637\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.96392\n",
            "Epoch 183/10000\n",
            "188/188 [==============================] - 12s 61ms/step - loss: 0.0957 - accuracy: 0.9753 - val_loss: 0.1344 - val_accuracy: 0.9645\n",
            "\n",
            "Epoch 00183: val_accuracy improved from 0.96392 to 0.96450, saving model to mnist_conv_best.h5\n",
            "Epoch 184/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0951 - accuracy: 0.9755 - val_loss: 0.1349 - val_accuracy: 0.9646\n",
            "\n",
            "Epoch 00184: val_accuracy improved from 0.96450 to 0.96458, saving model to mnist_conv_best.h5\n",
            "Epoch 185/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0943 - accuracy: 0.9757 - val_loss: 0.1341 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00185: val_accuracy improved from 0.96458 to 0.96492, saving model to mnist_conv_best.h5\n",
            "Epoch 186/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0937 - accuracy: 0.9758 - val_loss: 0.1344 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.96492\n",
            "Epoch 187/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0931 - accuracy: 0.9758 - val_loss: 0.1321 - val_accuracy: 0.9646\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.96492\n",
            "Epoch 188/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0926 - accuracy: 0.9763 - val_loss: 0.1321 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00188: val_accuracy improved from 0.96492 to 0.96533, saving model to mnist_conv_best.h5\n",
            "Epoch 189/10000\n",
            "188/188 [==============================] - 11s 61ms/step - loss: 0.0917 - accuracy: 0.9767 - val_loss: 0.1317 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.96533\n",
            "Epoch 190/10000\n",
            "188/188 [==============================] - 12s 61ms/step - loss: 0.0914 - accuracy: 0.9763 - val_loss: 0.1314 - val_accuracy: 0.9655\n",
            "\n",
            "Epoch 00190: val_accuracy improved from 0.96533 to 0.96550, saving model to mnist_conv_best.h5\n",
            "Epoch 191/10000\n",
            "188/188 [==============================] - 12s 61ms/step - loss: 0.0907 - accuracy: 0.9769 - val_loss: 0.1304 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.96550\n",
            "Epoch 192/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0899 - accuracy: 0.9771 - val_loss: 0.1297 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.96550\n",
            "Epoch 193/10000\n",
            "188/188 [==============================] - 12s 61ms/step - loss: 0.0895 - accuracy: 0.9770 - val_loss: 0.1291 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.96550\n",
            "Epoch 194/10000\n",
            "188/188 [==============================] - 12s 61ms/step - loss: 0.0889 - accuracy: 0.9772 - val_loss: 0.1306 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.96550\n",
            "Epoch 195/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0884 - accuracy: 0.9777 - val_loss: 0.1280 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00195: val_accuracy improved from 0.96550 to 0.96575, saving model to mnist_conv_best.h5\n",
            "Epoch 196/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0876 - accuracy: 0.9774 - val_loss: 0.1296 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.96575\n",
            "Epoch 197/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0873 - accuracy: 0.9778 - val_loss: 0.1282 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.96575\n",
            "Epoch 198/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0867 - accuracy: 0.9780 - val_loss: 0.1269 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00198: val_accuracy improved from 0.96575 to 0.96625, saving model to mnist_conv_best.h5\n",
            "Epoch 199/10000\n",
            "188/188 [==============================] - 12s 61ms/step - loss: 0.0861 - accuracy: 0.9776 - val_loss: 0.1264 - val_accuracy: 0.9655\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.96625\n",
            "Epoch 200/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0854 - accuracy: 0.9780 - val_loss: 0.1270 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.96625\n",
            "Epoch 201/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0849 - accuracy: 0.9783 - val_loss: 0.1258 - val_accuracy: 0.9665\n",
            "\n",
            "Epoch 00201: val_accuracy improved from 0.96625 to 0.96650, saving model to mnist_conv_best.h5\n",
            "Epoch 202/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0844 - accuracy: 0.9787 - val_loss: 0.1244 - val_accuracy: 0.9662\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.96650\n",
            "Epoch 203/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0840 - accuracy: 0.9785 - val_loss: 0.1249 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.96650\n",
            "Epoch 204/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0835 - accuracy: 0.9786 - val_loss: 0.1245 - val_accuracy: 0.9661\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.96650\n",
            "Epoch 205/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0829 - accuracy: 0.9788 - val_loss: 0.1236 - val_accuracy: 0.9658\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.96650\n",
            "Epoch 206/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0822 - accuracy: 0.9793 - val_loss: 0.1233 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00206: val_accuracy improved from 0.96650 to 0.96733, saving model to mnist_conv_best.h5\n",
            "Epoch 207/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0819 - accuracy: 0.9793 - val_loss: 0.1231 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.96733\n",
            "Epoch 208/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0814 - accuracy: 0.9793 - val_loss: 0.1219 - val_accuracy: 0.9663\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.96733\n",
            "Epoch 209/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0808 - accuracy: 0.9798 - val_loss: 0.1218 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.96733\n",
            "Epoch 210/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0805 - accuracy: 0.9797 - val_loss: 0.1209 - val_accuracy: 0.9669\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.96733\n",
            "Epoch 211/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0799 - accuracy: 0.9795 - val_loss: 0.1228 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.96733\n",
            "Epoch 212/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0795 - accuracy: 0.9799 - val_loss: 0.1216 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.96733\n",
            "Epoch 213/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0790 - accuracy: 0.9802 - val_loss: 0.1206 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.96733\n",
            "Epoch 214/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0785 - accuracy: 0.9805 - val_loss: 0.1201 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.96733\n",
            "Epoch 215/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0781 - accuracy: 0.9803 - val_loss: 0.1200 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00215: val_accuracy improved from 0.96733 to 0.96775, saving model to mnist_conv_best.h5\n",
            "Epoch 216/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0776 - accuracy: 0.9800 - val_loss: 0.1194 - val_accuracy: 0.9671\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.96775\n",
            "Epoch 217/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0773 - accuracy: 0.9806 - val_loss: 0.1184 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00217: val_accuracy improved from 0.96775 to 0.96817, saving model to mnist_conv_best.h5\n",
            "Epoch 218/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0765 - accuracy: 0.9806 - val_loss: 0.1188 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.96817\n",
            "Epoch 219/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0763 - accuracy: 0.9805 - val_loss: 0.1177 - val_accuracy: 0.9679\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.96817\n",
            "Epoch 220/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0759 - accuracy: 0.9811 - val_loss: 0.1181 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.96817\n",
            "Epoch 221/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0754 - accuracy: 0.9811 - val_loss: 0.1183 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.96817\n",
            "Epoch 222/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0750 - accuracy: 0.9812 - val_loss: 0.1168 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.96817\n",
            "Epoch 223/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0745 - accuracy: 0.9812 - val_loss: 0.1172 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.96817\n",
            "Epoch 224/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0741 - accuracy: 0.9813 - val_loss: 0.1172 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.96817\n",
            "Epoch 225/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0738 - accuracy: 0.9813 - val_loss: 0.1157 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00225: val_accuracy improved from 0.96817 to 0.96858, saving model to mnist_conv_best.h5\n",
            "Epoch 226/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0733 - accuracy: 0.9816 - val_loss: 0.1160 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.96858\n",
            "Epoch 227/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0731 - accuracy: 0.9817 - val_loss: 0.1150 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.96858\n",
            "Epoch 228/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0725 - accuracy: 0.9816 - val_loss: 0.1154 - val_accuracy: 0.9676\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.96858\n",
            "Epoch 229/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0721 - accuracy: 0.9820 - val_loss: 0.1143 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00229: val_accuracy improved from 0.96858 to 0.96867, saving model to mnist_conv_best.h5\n",
            "Epoch 230/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0718 - accuracy: 0.9820 - val_loss: 0.1140 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.96867\n",
            "Epoch 231/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0713 - accuracy: 0.9821 - val_loss: 0.1144 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.96867\n",
            "Epoch 232/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0710 - accuracy: 0.9820 - val_loss: 0.1136 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00232: val_accuracy improved from 0.96867 to 0.96925, saving model to mnist_conv_best.h5\n",
            "Epoch 233/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0706 - accuracy: 0.9824 - val_loss: 0.1139 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.96925\n",
            "Epoch 234/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0702 - accuracy: 0.9825 - val_loss: 0.1139 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.96925\n",
            "Epoch 235/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0699 - accuracy: 0.9825 - val_loss: 0.1138 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.96925\n",
            "Epoch 236/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0695 - accuracy: 0.9826 - val_loss: 0.1126 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.96925\n",
            "Epoch 237/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0691 - accuracy: 0.9828 - val_loss: 0.1124 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.96925\n",
            "Epoch 238/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0687 - accuracy: 0.9826 - val_loss: 0.1123 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.96925\n",
            "Epoch 239/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0684 - accuracy: 0.9830 - val_loss: 0.1111 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.96925\n",
            "Epoch 240/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0681 - accuracy: 0.9828 - val_loss: 0.1114 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00240: val_accuracy improved from 0.96925 to 0.96942, saving model to mnist_conv_best.h5\n",
            "Epoch 241/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0677 - accuracy: 0.9834 - val_loss: 0.1119 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.96942\n",
            "Epoch 242/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0674 - accuracy: 0.9829 - val_loss: 0.1106 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.96942\n",
            "Epoch 243/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0669 - accuracy: 0.9834 - val_loss: 0.1113 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.96942\n",
            "Epoch 244/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0666 - accuracy: 0.9834 - val_loss: 0.1094 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.96942\n",
            "Epoch 245/10000\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0663 - accuracy: 0.9835 - val_loss: 0.1098 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.96942\n",
            "Epoch 246/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0660 - accuracy: 0.9837 - val_loss: 0.1092 - val_accuracy: 0.9689\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.96942\n",
            "Epoch 247/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0657 - accuracy: 0.9840 - val_loss: 0.1101 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.96942\n",
            "Epoch 248/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0655 - accuracy: 0.9836 - val_loss: 0.1091 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.96942\n",
            "Epoch 249/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0651 - accuracy: 0.9838 - val_loss: 0.1086 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.96942\n",
            "Epoch 250/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0647 - accuracy: 0.9842 - val_loss: 0.1084 - val_accuracy: 0.9698\n",
            "\n",
            "Epoch 00250: val_accuracy improved from 0.96942 to 0.96983, saving model to mnist_conv_best.h5\n",
            "Epoch 251/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0644 - accuracy: 0.9843 - val_loss: 0.1084 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.96983\n",
            "Epoch 252/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0641 - accuracy: 0.9843 - val_loss: 0.1081 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.96983\n",
            "Epoch 253/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0638 - accuracy: 0.9843 - val_loss: 0.1075 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.96983\n",
            "Epoch 254/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0635 - accuracy: 0.9847 - val_loss: 0.1078 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.96983\n",
            "Epoch 255/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0631 - accuracy: 0.9846 - val_loss: 0.1080 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.96983\n",
            "Epoch 256/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0629 - accuracy: 0.9846 - val_loss: 0.1095 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.96983\n",
            "Epoch 257/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0626 - accuracy: 0.9848 - val_loss: 0.1066 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.96983\n",
            "Epoch 258/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0623 - accuracy: 0.9851 - val_loss: 0.1065 - val_accuracy: 0.9698\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.96983\n",
            "Epoch 259/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0620 - accuracy: 0.9848 - val_loss: 0.1064 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00259: val_accuracy improved from 0.96983 to 0.97017, saving model to mnist_conv_best.h5\n",
            "Epoch 260/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0617 - accuracy: 0.9851 - val_loss: 0.1064 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.97017\n",
            "Epoch 261/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0614 - accuracy: 0.9854 - val_loss: 0.1059 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.97017\n",
            "Epoch 262/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0611 - accuracy: 0.9851 - val_loss: 0.1066 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.97017\n",
            "Epoch 263/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0608 - accuracy: 0.9851 - val_loss: 0.1055 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.97017\n",
            "Epoch 264/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0605 - accuracy: 0.9852 - val_loss: 0.1047 - val_accuracy: 0.9693\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.97017\n",
            "Epoch 265/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0603 - accuracy: 0.9852 - val_loss: 0.1050 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.97017\n",
            "Epoch 266/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0601 - accuracy: 0.9851 - val_loss: 0.1050 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.97017\n",
            "Epoch 267/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0599 - accuracy: 0.9852 - val_loss: 0.1047 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.97017\n",
            "Epoch 268/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0594 - accuracy: 0.9853 - val_loss: 0.1046 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.97017\n",
            "Epoch 269/10000\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0593 - accuracy: 0.9853 - val_loss: 0.1047 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.97017\n",
            "Epoch 00269: early stopping\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0702 - accuracy: 0.9820\n",
            "Accuracy for the training set: 0.9820166826248169\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0931 - accuracy: 0.9714\n",
            "Accuracy for the testing set: 0.9714000225067139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution (Conv2D)         (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                125450    \n",
            "=================================================================\n",
            "Total params: 125,610\n",
            "Trainable params: 125,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.1767 - accuracy: 0.7377 - val_loss: 0.5247 - val_accuracy: 0.8654\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.86542, saving model to mnist_conv_best.h5\n",
            "Epoch 2/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.4423 - accuracy: 0.8804 - val_loss: 0.4025 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.86542 to 0.88817, saving model to mnist_conv_best.h5\n",
            "Epoch 3/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.3759 - accuracy: 0.8929 - val_loss: 0.3643 - val_accuracy: 0.8959\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88817 to 0.89592, saving model to mnist_conv_best.h5\n",
            "Epoch 4/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.3495 - accuracy: 0.8994 - val_loss: 0.3484 - val_accuracy: 0.9014\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89592 to 0.90142, saving model to mnist_conv_best.h5\n",
            "Epoch 5/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.3337 - accuracy: 0.9034 - val_loss: 0.3345 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90142 to 0.90558, saving model to mnist_conv_best.h5\n",
            "Epoch 6/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.3221 - accuracy: 0.9071 - val_loss: 0.3285 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90558 to 0.90742, saving model to mnist_conv_best.h5\n",
            "Epoch 7/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.3136 - accuracy: 0.9102 - val_loss: 0.3211 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90742 to 0.91117, saving model to mnist_conv_best.h5\n",
            "Epoch 8/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.3064 - accuracy: 0.9121 - val_loss: 0.3144 - val_accuracy: 0.9129\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.91117 to 0.91292, saving model to mnist_conv_best.h5\n",
            "Epoch 9/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.3007 - accuracy: 0.9140 - val_loss: 0.3114 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.91292 to 0.91483, saving model to mnist_conv_best.h5\n",
            "Epoch 10/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2962 - accuracy: 0.9158 - val_loss: 0.3124 - val_accuracy: 0.9140\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.91483\n",
            "Epoch 11/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2922 - accuracy: 0.9170 - val_loss: 0.3036 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.91483 to 0.91725, saving model to mnist_conv_best.h5\n",
            "Epoch 12/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2886 - accuracy: 0.9177 - val_loss: 0.2983 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.91725 to 0.91733, saving model to mnist_conv_best.h5\n",
            "Epoch 13/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2857 - accuracy: 0.9189 - val_loss: 0.3023 - val_accuracy: 0.9149\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91733\n",
            "Epoch 14/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2831 - accuracy: 0.9199 - val_loss: 0.2956 - val_accuracy: 0.9196\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.91733 to 0.91958, saving model to mnist_conv_best.h5\n",
            "Epoch 15/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2804 - accuracy: 0.9203 - val_loss: 0.2922 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.91958 to 0.92092, saving model to mnist_conv_best.h5\n",
            "Epoch 16/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2780 - accuracy: 0.9215 - val_loss: 0.2921 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.92092\n",
            "Epoch 17/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2764 - accuracy: 0.9218 - val_loss: 0.2916 - val_accuracy: 0.9205\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.92092\n",
            "Epoch 18/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2737 - accuracy: 0.9224 - val_loss: 0.2869 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.92092 to 0.92108, saving model to mnist_conv_best.h5\n",
            "Epoch 19/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2718 - accuracy: 0.9226 - val_loss: 0.2866 - val_accuracy: 0.9216\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.92108 to 0.92158, saving model to mnist_conv_best.h5\n",
            "Epoch 20/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2696 - accuracy: 0.9242 - val_loss: 0.2856 - val_accuracy: 0.9222\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.92158 to 0.92225, saving model to mnist_conv_best.h5\n",
            "Epoch 21/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2677 - accuracy: 0.9245 - val_loss: 0.2838 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.92225\n",
            "Epoch 22/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2654 - accuracy: 0.9250 - val_loss: 0.2800 - val_accuracy: 0.9222\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.92225\n",
            "Epoch 23/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2636 - accuracy: 0.9260 - val_loss: 0.2912 - val_accuracy: 0.9184\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.92225\n",
            "Epoch 24/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2620 - accuracy: 0.9260 - val_loss: 0.2777 - val_accuracy: 0.9243\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.92225 to 0.92425, saving model to mnist_conv_best.h5\n",
            "Epoch 25/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2593 - accuracy: 0.9265 - val_loss: 0.2787 - val_accuracy: 0.9224\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.92425\n",
            "Epoch 26/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2576 - accuracy: 0.9277 - val_loss: 0.2731 - val_accuracy: 0.9249\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.92425 to 0.92492, saving model to mnist_conv_best.h5\n",
            "Epoch 27/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2559 - accuracy: 0.9284 - val_loss: 0.2710 - val_accuracy: 0.9247\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.92492\n",
            "Epoch 28/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.2534 - accuracy: 0.9279 - val_loss: 0.2747 - val_accuracy: 0.9217\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.92492\n",
            "Epoch 29/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2516 - accuracy: 0.9290 - val_loss: 0.2695 - val_accuracy: 0.9252\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.92492 to 0.92517, saving model to mnist_conv_best.h5\n",
            "Epoch 30/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.2495 - accuracy: 0.9300 - val_loss: 0.2690 - val_accuracy: 0.9262\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.92517 to 0.92625, saving model to mnist_conv_best.h5\n",
            "Epoch 31/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2476 - accuracy: 0.9310 - val_loss: 0.2655 - val_accuracy: 0.9270\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.92625 to 0.92700, saving model to mnist_conv_best.h5\n",
            "Epoch 32/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2454 - accuracy: 0.9313 - val_loss: 0.2644 - val_accuracy: 0.9268\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.92700\n",
            "Epoch 33/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2431 - accuracy: 0.9315 - val_loss: 0.2651 - val_accuracy: 0.9269\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.92700\n",
            "Epoch 34/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2409 - accuracy: 0.9325 - val_loss: 0.2584 - val_accuracy: 0.9282\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.92700 to 0.92817, saving model to mnist_conv_best.h5\n",
            "Epoch 35/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2382 - accuracy: 0.9337 - val_loss: 0.2584 - val_accuracy: 0.9283\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.92817 to 0.92833, saving model to mnist_conv_best.h5\n",
            "Epoch 36/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2361 - accuracy: 0.9344 - val_loss: 0.2546 - val_accuracy: 0.9298\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.92833 to 0.92975, saving model to mnist_conv_best.h5\n",
            "Epoch 37/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2341 - accuracy: 0.9348 - val_loss: 0.2504 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.92975 to 0.93158, saving model to mnist_conv_best.h5\n",
            "Epoch 38/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.2316 - accuracy: 0.9356 - val_loss: 0.2500 - val_accuracy: 0.9305\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.93158\n",
            "Epoch 39/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2288 - accuracy: 0.9366 - val_loss: 0.2485 - val_accuracy: 0.9315\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.93158\n",
            "Epoch 40/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2263 - accuracy: 0.9376 - val_loss: 0.2552 - val_accuracy: 0.9311\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.93158\n",
            "Epoch 41/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.2240 - accuracy: 0.9381 - val_loss: 0.2425 - val_accuracy: 0.9328\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.93158 to 0.93283, saving model to mnist_conv_best.h5\n",
            "Epoch 42/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2209 - accuracy: 0.9388 - val_loss: 0.2451 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.93283\n",
            "Epoch 43/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2179 - accuracy: 0.9404 - val_loss: 0.2375 - val_accuracy: 0.9352\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.93283 to 0.93525, saving model to mnist_conv_best.h5\n",
            "Epoch 44/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2156 - accuracy: 0.9407 - val_loss: 0.2371 - val_accuracy: 0.9359\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.93525 to 0.93592, saving model to mnist_conv_best.h5\n",
            "Epoch 45/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2131 - accuracy: 0.9408 - val_loss: 0.2329 - val_accuracy: 0.9362\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.93592 to 0.93625, saving model to mnist_conv_best.h5\n",
            "Epoch 46/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.2105 - accuracy: 0.9422 - val_loss: 0.2284 - val_accuracy: 0.9380\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.93625 to 0.93800, saving model to mnist_conv_best.h5\n",
            "Epoch 47/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.2079 - accuracy: 0.9430 - val_loss: 0.2257 - val_accuracy: 0.9382\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.93800 to 0.93817, saving model to mnist_conv_best.h5\n",
            "Epoch 48/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.2046 - accuracy: 0.9433 - val_loss: 0.2249 - val_accuracy: 0.9402\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.93817 to 0.94025, saving model to mnist_conv_best.h5\n",
            "Epoch 49/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.2020 - accuracy: 0.9446 - val_loss: 0.2223 - val_accuracy: 0.9393\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.94025\n",
            "Epoch 50/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.1992 - accuracy: 0.9456 - val_loss: 0.2211 - val_accuracy: 0.9416\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.94025 to 0.94158, saving model to mnist_conv_best.h5\n",
            "Epoch 51/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1966 - accuracy: 0.9461 - val_loss: 0.2156 - val_accuracy: 0.9417\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.94158 to 0.94167, saving model to mnist_conv_best.h5\n",
            "Epoch 52/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1936 - accuracy: 0.9470 - val_loss: 0.2138 - val_accuracy: 0.9424\n",
            "\n",
            "Epoch 00052: val_accuracy improved from 0.94167 to 0.94242, saving model to mnist_conv_best.h5\n",
            "Epoch 53/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1910 - accuracy: 0.9479 - val_loss: 0.2123 - val_accuracy: 0.9438\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.94242 to 0.94375, saving model to mnist_conv_best.h5\n",
            "Epoch 54/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.1883 - accuracy: 0.9488 - val_loss: 0.2097 - val_accuracy: 0.9415\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.94375\n",
            "Epoch 55/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1857 - accuracy: 0.9497 - val_loss: 0.2058 - val_accuracy: 0.9436\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.94375\n",
            "Epoch 56/10000\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.1832 - accuracy: 0.9497 - val_loss: 0.2033 - val_accuracy: 0.9450\n",
            "\n",
            "Epoch 00056: val_accuracy improved from 0.94375 to 0.94500, saving model to mnist_conv_best.h5\n",
            "Epoch 57/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1803 - accuracy: 0.9512 - val_loss: 0.2013 - val_accuracy: 0.9455\n",
            "\n",
            "Epoch 00057: val_accuracy improved from 0.94500 to 0.94550, saving model to mnist_conv_best.h5\n",
            "Epoch 58/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1779 - accuracy: 0.9516 - val_loss: 0.2002 - val_accuracy: 0.9449\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.94550\n",
            "Epoch 59/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1753 - accuracy: 0.9525 - val_loss: 0.1964 - val_accuracy: 0.9459\n",
            "\n",
            "Epoch 00059: val_accuracy improved from 0.94550 to 0.94592, saving model to mnist_conv_best.h5\n",
            "Epoch 60/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1725 - accuracy: 0.9537 - val_loss: 0.1940 - val_accuracy: 0.9479\n",
            "\n",
            "Epoch 00060: val_accuracy improved from 0.94592 to 0.94792, saving model to mnist_conv_best.h5\n",
            "Epoch 61/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1702 - accuracy: 0.9539 - val_loss: 0.1917 - val_accuracy: 0.9479\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.94792\n",
            "Epoch 62/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1678 - accuracy: 0.9547 - val_loss: 0.1898 - val_accuracy: 0.9479\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.94792\n",
            "Epoch 63/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1656 - accuracy: 0.9550 - val_loss: 0.1873 - val_accuracy: 0.9481\n",
            "\n",
            "Epoch 00063: val_accuracy improved from 0.94792 to 0.94808, saving model to mnist_conv_best.h5\n",
            "Epoch 64/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1634 - accuracy: 0.9561 - val_loss: 0.1846 - val_accuracy: 0.9488\n",
            "\n",
            "Epoch 00064: val_accuracy improved from 0.94808 to 0.94875, saving model to mnist_conv_best.h5\n",
            "Epoch 65/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1610 - accuracy: 0.9566 - val_loss: 0.1816 - val_accuracy: 0.9508\n",
            "\n",
            "Epoch 00065: val_accuracy improved from 0.94875 to 0.95083, saving model to mnist_conv_best.h5\n",
            "Epoch 66/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1587 - accuracy: 0.9577 - val_loss: 0.1795 - val_accuracy: 0.9518\n",
            "\n",
            "Epoch 00066: val_accuracy improved from 0.95083 to 0.95183, saving model to mnist_conv_best.h5\n",
            "Epoch 67/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1564 - accuracy: 0.9578 - val_loss: 0.1795 - val_accuracy: 0.9519\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.95183 to 0.95192, saving model to mnist_conv_best.h5\n",
            "Epoch 68/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1543 - accuracy: 0.9584 - val_loss: 0.1766 - val_accuracy: 0.9523\n",
            "\n",
            "Epoch 00068: val_accuracy improved from 0.95192 to 0.95225, saving model to mnist_conv_best.h5\n",
            "Epoch 69/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1524 - accuracy: 0.9587 - val_loss: 0.1747 - val_accuracy: 0.9526\n",
            "\n",
            "Epoch 00069: val_accuracy improved from 0.95225 to 0.95258, saving model to mnist_conv_best.h5\n",
            "Epoch 70/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1506 - accuracy: 0.9592 - val_loss: 0.1733 - val_accuracy: 0.9527\n",
            "\n",
            "Epoch 00070: val_accuracy improved from 0.95258 to 0.95267, saving model to mnist_conv_best.h5\n",
            "Epoch 71/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1485 - accuracy: 0.9600 - val_loss: 0.1697 - val_accuracy: 0.9542\n",
            "\n",
            "Epoch 00071: val_accuracy improved from 0.95267 to 0.95425, saving model to mnist_conv_best.h5\n",
            "Epoch 72/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1463 - accuracy: 0.9611 - val_loss: 0.1687 - val_accuracy: 0.9544\n",
            "\n",
            "Epoch 00072: val_accuracy improved from 0.95425 to 0.95442, saving model to mnist_conv_best.h5\n",
            "Epoch 73/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1445 - accuracy: 0.9616 - val_loss: 0.1702 - val_accuracy: 0.9536\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.95442\n",
            "Epoch 74/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1425 - accuracy: 0.9617 - val_loss: 0.1648 - val_accuracy: 0.9546\n",
            "\n",
            "Epoch 00074: val_accuracy improved from 0.95442 to 0.95458, saving model to mnist_conv_best.h5\n",
            "Epoch 75/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1407 - accuracy: 0.9620 - val_loss: 0.1637 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00075: val_accuracy improved from 0.95458 to 0.95550, saving model to mnist_conv_best.h5\n",
            "Epoch 76/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1389 - accuracy: 0.9634 - val_loss: 0.1658 - val_accuracy: 0.9538\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.95550\n",
            "Epoch 77/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1374 - accuracy: 0.9631 - val_loss: 0.1593 - val_accuracy: 0.9571\n",
            "\n",
            "Epoch 00077: val_accuracy improved from 0.95550 to 0.95708, saving model to mnist_conv_best.h5\n",
            "Epoch 78/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1358 - accuracy: 0.9639 - val_loss: 0.1624 - val_accuracy: 0.9545\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.95708\n",
            "Epoch 79/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1341 - accuracy: 0.9643 - val_loss: 0.1561 - val_accuracy: 0.9580\n",
            "\n",
            "Epoch 00079: val_accuracy improved from 0.95708 to 0.95800, saving model to mnist_conv_best.h5\n",
            "Epoch 80/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1322 - accuracy: 0.9646 - val_loss: 0.1562 - val_accuracy: 0.9573\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.95800\n",
            "Epoch 81/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1309 - accuracy: 0.9654 - val_loss: 0.1549 - val_accuracy: 0.9576\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.95800\n",
            "Epoch 82/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1292 - accuracy: 0.9655 - val_loss: 0.1522 - val_accuracy: 0.9588\n",
            "\n",
            "Epoch 00082: val_accuracy improved from 0.95800 to 0.95883, saving model to mnist_conv_best.h5\n",
            "Epoch 83/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1277 - accuracy: 0.9660 - val_loss: 0.1524 - val_accuracy: 0.9590\n",
            "\n",
            "Epoch 00083: val_accuracy improved from 0.95883 to 0.95900, saving model to mnist_conv_best.h5\n",
            "Epoch 84/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1262 - accuracy: 0.9670 - val_loss: 0.1504 - val_accuracy: 0.9589\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.95900\n",
            "Epoch 85/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1248 - accuracy: 0.9670 - val_loss: 0.1490 - val_accuracy: 0.9599\n",
            "\n",
            "Epoch 00085: val_accuracy improved from 0.95900 to 0.95992, saving model to mnist_conv_best.h5\n",
            "Epoch 86/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1235 - accuracy: 0.9673 - val_loss: 0.1478 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00086: val_accuracy improved from 0.95992 to 0.96017, saving model to mnist_conv_best.h5\n",
            "Epoch 87/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1224 - accuracy: 0.9674 - val_loss: 0.1465 - val_accuracy: 0.9598\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.96017\n",
            "Epoch 88/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1207 - accuracy: 0.9682 - val_loss: 0.1458 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.96017\n",
            "Epoch 89/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1194 - accuracy: 0.9688 - val_loss: 0.1432 - val_accuracy: 0.9605\n",
            "\n",
            "Epoch 00089: val_accuracy improved from 0.96017 to 0.96050, saving model to mnist_conv_best.h5\n",
            "Epoch 90/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1183 - accuracy: 0.9694 - val_loss: 0.1417 - val_accuracy: 0.9619\n",
            "\n",
            "Epoch 00090: val_accuracy improved from 0.96050 to 0.96192, saving model to mnist_conv_best.h5\n",
            "Epoch 91/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1169 - accuracy: 0.9693 - val_loss: 0.1418 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.96192\n",
            "Epoch 92/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1156 - accuracy: 0.9693 - val_loss: 0.1409 - val_accuracy: 0.9605\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.96192\n",
            "Epoch 93/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1146 - accuracy: 0.9699 - val_loss: 0.1398 - val_accuracy: 0.9624\n",
            "\n",
            "Epoch 00093: val_accuracy improved from 0.96192 to 0.96242, saving model to mnist_conv_best.h5\n",
            "Epoch 94/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1133 - accuracy: 0.9704 - val_loss: 0.1384 - val_accuracy: 0.9618\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.96242\n",
            "Epoch 95/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1123 - accuracy: 0.9705 - val_loss: 0.1379 - val_accuracy: 0.9622\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.96242\n",
            "Epoch 96/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1109 - accuracy: 0.9712 - val_loss: 0.1364 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00096: val_accuracy improved from 0.96242 to 0.96375, saving model to mnist_conv_best.h5\n",
            "Epoch 97/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1099 - accuracy: 0.9716 - val_loss: 0.1362 - val_accuracy: 0.9635\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.96375\n",
            "Epoch 98/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1090 - accuracy: 0.9713 - val_loss: 0.1339 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.96375\n",
            "Epoch 99/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.1079 - accuracy: 0.9719 - val_loss: 0.1348 - val_accuracy: 0.9625\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.96375\n",
            "Epoch 100/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1067 - accuracy: 0.9722 - val_loss: 0.1319 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00100: val_accuracy improved from 0.96375 to 0.96442, saving model to mnist_conv_best.h5\n",
            "Epoch 101/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1059 - accuracy: 0.9724 - val_loss: 0.1335 - val_accuracy: 0.9636\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.96442\n",
            "Epoch 102/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1048 - accuracy: 0.9727 - val_loss: 0.1303 - val_accuracy: 0.9644\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.96442\n",
            "Epoch 103/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1040 - accuracy: 0.9722 - val_loss: 0.1310 - val_accuracy: 0.9635\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.96442\n",
            "Epoch 104/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1029 - accuracy: 0.9730 - val_loss: 0.1287 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00104: val_accuracy improved from 0.96442 to 0.96542, saving model to mnist_conv_best.h5\n",
            "Epoch 105/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1019 - accuracy: 0.9732 - val_loss: 0.1293 - val_accuracy: 0.9646\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.96542\n",
            "Epoch 106/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1010 - accuracy: 0.9732 - val_loss: 0.1262 - val_accuracy: 0.9654\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.96542\n",
            "Epoch 107/10000\n",
            "188/188 [==============================] - 12s 67ms/step - loss: 0.1001 - accuracy: 0.9737 - val_loss: 0.1259 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.96542\n",
            "Epoch 108/10000\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.0992 - accuracy: 0.9742 - val_loss: 0.1266 - val_accuracy: 0.9653\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.96542\n",
            "Epoch 109/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0983 - accuracy: 0.9742 - val_loss: 0.1256 - val_accuracy: 0.9658\n",
            "\n",
            "Epoch 00109: val_accuracy improved from 0.96542 to 0.96583, saving model to mnist_conv_best.h5\n",
            "Epoch 110/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0975 - accuracy: 0.9740 - val_loss: 0.1235 - val_accuracy: 0.9658\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.96583\n",
            "Epoch 111/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0967 - accuracy: 0.9746 - val_loss: 0.1227 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00111: val_accuracy improved from 0.96583 to 0.96592, saving model to mnist_conv_best.h5\n",
            "Epoch 112/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0959 - accuracy: 0.9749 - val_loss: 0.1226 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.96592\n",
            "Epoch 113/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0949 - accuracy: 0.9747 - val_loss: 0.1227 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.96592\n",
            "Epoch 114/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0941 - accuracy: 0.9752 - val_loss: 0.1210 - val_accuracy: 0.9660\n",
            "\n",
            "Epoch 00114: val_accuracy improved from 0.96592 to 0.96600, saving model to mnist_conv_best.h5\n",
            "Epoch 115/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0933 - accuracy: 0.9754 - val_loss: 0.1200 - val_accuracy: 0.9667\n",
            "\n",
            "Epoch 00115: val_accuracy improved from 0.96600 to 0.96667, saving model to mnist_conv_best.h5\n",
            "Epoch 116/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0925 - accuracy: 0.9758 - val_loss: 0.1197 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00116: val_accuracy improved from 0.96667 to 0.96700, saving model to mnist_conv_best.h5\n",
            "Epoch 117/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0916 - accuracy: 0.9758 - val_loss: 0.1199 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.96700\n",
            "Epoch 118/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0910 - accuracy: 0.9760 - val_loss: 0.1180 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.96700\n",
            "Epoch 119/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0903 - accuracy: 0.9764 - val_loss: 0.1189 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.96700\n",
            "Epoch 120/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0898 - accuracy: 0.9767 - val_loss: 0.1161 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00120: val_accuracy improved from 0.96700 to 0.96725, saving model to mnist_conv_best.h5\n",
            "Epoch 121/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0889 - accuracy: 0.9764 - val_loss: 0.1170 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00121: val_accuracy improved from 0.96725 to 0.96817, saving model to mnist_conv_best.h5\n",
            "Epoch 122/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0882 - accuracy: 0.9773 - val_loss: 0.1157 - val_accuracy: 0.9662\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.96817\n",
            "Epoch 123/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0875 - accuracy: 0.9769 - val_loss: 0.1150 - val_accuracy: 0.9669\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.96817\n",
            "Epoch 124/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0869 - accuracy: 0.9775 - val_loss: 0.1148 - val_accuracy: 0.9677\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.96817\n",
            "Epoch 125/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0862 - accuracy: 0.9775 - val_loss: 0.1147 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.96817\n",
            "Epoch 126/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0856 - accuracy: 0.9778 - val_loss: 0.1133 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.96817\n",
            "Epoch 127/10000\n",
            "188/188 [==============================] - 12s 67ms/step - loss: 0.0848 - accuracy: 0.9777 - val_loss: 0.1122 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.96817\n",
            "Epoch 128/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0842 - accuracy: 0.9782 - val_loss: 0.1120 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00128: val_accuracy improved from 0.96817 to 0.96825, saving model to mnist_conv_best.h5\n",
            "Epoch 129/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0835 - accuracy: 0.9784 - val_loss: 0.1122 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.96825\n",
            "Epoch 130/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0829 - accuracy: 0.9786 - val_loss: 0.1118 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00130: val_accuracy improved from 0.96825 to 0.96858, saving model to mnist_conv_best.h5\n",
            "Epoch 131/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0823 - accuracy: 0.9785 - val_loss: 0.1119 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.96858\n",
            "Epoch 132/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0817 - accuracy: 0.9783 - val_loss: 0.1110 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.96858\n",
            "Epoch 133/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0811 - accuracy: 0.9786 - val_loss: 0.1117 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.96858\n",
            "Epoch 134/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0806 - accuracy: 0.9790 - val_loss: 0.1097 - val_accuracy: 0.9681\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.96858\n",
            "Epoch 135/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0799 - accuracy: 0.9795 - val_loss: 0.1090 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00135: val_accuracy improved from 0.96858 to 0.96883, saving model to mnist_conv_best.h5\n",
            "Epoch 136/10000\n",
            "188/188 [==============================] - 12s 67ms/step - loss: 0.0794 - accuracy: 0.9792 - val_loss: 0.1079 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.96883\n",
            "Epoch 137/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0788 - accuracy: 0.9795 - val_loss: 0.1080 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.96883\n",
            "Epoch 138/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0782 - accuracy: 0.9796 - val_loss: 0.1077 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.96883\n",
            "Epoch 139/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0776 - accuracy: 0.9797 - val_loss: 0.1086 - val_accuracy: 0.9687\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.96883\n",
            "Epoch 140/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0771 - accuracy: 0.9799 - val_loss: 0.1068 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.96883\n",
            "Epoch 141/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0765 - accuracy: 0.9798 - val_loss: 0.1064 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.96883\n",
            "Epoch 142/10000\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0760 - accuracy: 0.9803 - val_loss: 0.1050 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.96883\n",
            "Epoch 143/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0755 - accuracy: 0.9803 - val_loss: 0.1059 - val_accuracy: 0.9690\n",
            "\n",
            "Epoch 00143: val_accuracy improved from 0.96883 to 0.96900, saving model to mnist_conv_best.h5\n",
            "Epoch 144/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0749 - accuracy: 0.9802 - val_loss: 0.1040 - val_accuracy: 0.9699\n",
            "\n",
            "Epoch 00144: val_accuracy improved from 0.96900 to 0.96992, saving model to mnist_conv_best.h5\n",
            "Epoch 145/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0744 - accuracy: 0.9804 - val_loss: 0.1039 - val_accuracy: 0.9696\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.96992\n",
            "Epoch 146/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0740 - accuracy: 0.9805 - val_loss: 0.1036 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00146: val_accuracy improved from 0.96992 to 0.97067, saving model to mnist_conv_best.h5\n",
            "Epoch 147/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0734 - accuracy: 0.9805 - val_loss: 0.1032 - val_accuracy: 0.9698\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.97067\n",
            "Epoch 148/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0730 - accuracy: 0.9810 - val_loss: 0.1044 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.97067\n",
            "Epoch 149/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0725 - accuracy: 0.9811 - val_loss: 0.1027 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.97067\n",
            "Epoch 150/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0720 - accuracy: 0.9810 - val_loss: 0.1039 - val_accuracy: 0.9699\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.97067\n",
            "Epoch 151/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0715 - accuracy: 0.9813 - val_loss: 0.1026 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.97067\n",
            "Epoch 152/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0710 - accuracy: 0.9814 - val_loss: 0.1027 - val_accuracy: 0.9698\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.97067\n",
            "Epoch 153/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0707 - accuracy: 0.9816 - val_loss: 0.1016 - val_accuracy: 0.9699\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.97067\n",
            "Epoch 154/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0702 - accuracy: 0.9818 - val_loss: 0.1007 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.97067\n",
            "Epoch 155/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0698 - accuracy: 0.9820 - val_loss: 0.0999 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.97067\n",
            "Epoch 156/10000\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0693 - accuracy: 0.9820 - val_loss: 0.1004 - val_accuracy: 0.9705\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.97067\n",
            "Epoch 00156: early stopping\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0795 - accuracy: 0.9789\n",
            "Accuracy for the training set: 0.9788833260536194\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0879 - accuracy: 0.9738\n",
            "Accuracy for the testing set: 0.973800003528595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution (Conv2D)         (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                125450    \n",
            "=================================================================\n",
            "Total params: 125,722\n",
            "Trainable params: 125,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "188/188 [==============================] - 15s 76ms/step - loss: 1.1607 - accuracy: 0.7306 - val_loss: 0.5031 - val_accuracy: 0.8668\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.86683, saving model to mnist_conv_best.h5\n",
            "Epoch 2/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.4310 - accuracy: 0.8809 - val_loss: 0.3905 - val_accuracy: 0.8912\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.86683 to 0.89117, saving model to mnist_conv_best.h5\n",
            "Epoch 3/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.3703 - accuracy: 0.8952 - val_loss: 0.3591 - val_accuracy: 0.8987\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89117 to 0.89867, saving model to mnist_conv_best.h5\n",
            "Epoch 4/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.3455 - accuracy: 0.9003 - val_loss: 0.3434 - val_accuracy: 0.9043\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89867 to 0.90433, saving model to mnist_conv_best.h5\n",
            "Epoch 5/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.3299 - accuracy: 0.9049 - val_loss: 0.3308 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90433 to 0.90742, saving model to mnist_conv_best.h5\n",
            "Epoch 6/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.3190 - accuracy: 0.9084 - val_loss: 0.3241 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90742 to 0.91017, saving model to mnist_conv_best.h5\n",
            "Epoch 7/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.3100 - accuracy: 0.9108 - val_loss: 0.3162 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.91017\n",
            "Epoch 8/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.3033 - accuracy: 0.9130 - val_loss: 0.3124 - val_accuracy: 0.9136\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.91017 to 0.91358, saving model to mnist_conv_best.h5\n",
            "Epoch 9/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.2981 - accuracy: 0.9144 - val_loss: 0.3043 - val_accuracy: 0.9149\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.91358 to 0.91492, saving model to mnist_conv_best.h5\n",
            "Epoch 10/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2929 - accuracy: 0.9158 - val_loss: 0.3024 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.91492 to 0.91700, saving model to mnist_conv_best.h5\n",
            "Epoch 11/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.2883 - accuracy: 0.9177 - val_loss: 0.3004 - val_accuracy: 0.9178\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.91700 to 0.91783, saving model to mnist_conv_best.h5\n",
            "Epoch 12/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.2844 - accuracy: 0.9184 - val_loss: 0.2958 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.91783\n",
            "Epoch 13/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.2802 - accuracy: 0.9203 - val_loss: 0.2885 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.91783 to 0.92108, saving model to mnist_conv_best.h5\n",
            "Epoch 14/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.2772 - accuracy: 0.9209 - val_loss: 0.2877 - val_accuracy: 0.9197\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.92108\n",
            "Epoch 15/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2732 - accuracy: 0.9219 - val_loss: 0.2830 - val_accuracy: 0.9231\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.92108 to 0.92308, saving model to mnist_conv_best.h5\n",
            "Epoch 16/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.2693 - accuracy: 0.9232 - val_loss: 0.2809 - val_accuracy: 0.9215\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.92308\n",
            "Epoch 17/10000\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 0.2661 - accuracy: 0.9245 - val_loss: 0.2881 - val_accuracy: 0.9188\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.92308\n",
            "Epoch 18/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2624 - accuracy: 0.9256 - val_loss: 0.2715 - val_accuracy: 0.9252\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.92308 to 0.92517, saving model to mnist_conv_best.h5\n",
            "Epoch 19/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2581 - accuracy: 0.9258 - val_loss: 0.2696 - val_accuracy: 0.9275\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.92517 to 0.92750, saving model to mnist_conv_best.h5\n",
            "Epoch 20/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2545 - accuracy: 0.9277 - val_loss: 0.2704 - val_accuracy: 0.9257\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.92750\n",
            "Epoch 21/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2507 - accuracy: 0.9294 - val_loss: 0.2642 - val_accuracy: 0.9284\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.92750 to 0.92842, saving model to mnist_conv_best.h5\n",
            "Epoch 22/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2464 - accuracy: 0.9300 - val_loss: 0.2583 - val_accuracy: 0.9300\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.92842 to 0.93000, saving model to mnist_conv_best.h5\n",
            "Epoch 23/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2431 - accuracy: 0.9319 - val_loss: 0.2584 - val_accuracy: 0.9291\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93000\n",
            "Epoch 24/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2384 - accuracy: 0.9330 - val_loss: 0.2525 - val_accuracy: 0.9313\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.93000 to 0.93133, saving model to mnist_conv_best.h5\n",
            "Epoch 25/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2341 - accuracy: 0.9339 - val_loss: 0.2471 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.93133 to 0.93333, saving model to mnist_conv_best.h5\n",
            "Epoch 26/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2298 - accuracy: 0.9358 - val_loss: 0.2443 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.93333 to 0.93342, saving model to mnist_conv_best.h5\n",
            "Epoch 27/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2254 - accuracy: 0.9367 - val_loss: 0.2379 - val_accuracy: 0.9362\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.93342 to 0.93625, saving model to mnist_conv_best.h5\n",
            "Epoch 28/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.2206 - accuracy: 0.9381 - val_loss: 0.2364 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93625\n",
            "Epoch 29/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2167 - accuracy: 0.9395 - val_loss: 0.2292 - val_accuracy: 0.9378\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.93625 to 0.93783, saving model to mnist_conv_best.h5\n",
            "Epoch 30/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.2122 - accuracy: 0.9407 - val_loss: 0.2257 - val_accuracy: 0.9385\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.93783 to 0.93850, saving model to mnist_conv_best.h5\n",
            "Epoch 31/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.2074 - accuracy: 0.9423 - val_loss: 0.2213 - val_accuracy: 0.9393\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.93850 to 0.93933, saving model to mnist_conv_best.h5\n",
            "Epoch 32/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.2027 - accuracy: 0.9436 - val_loss: 0.2213 - val_accuracy: 0.9397\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.93933 to 0.93967, saving model to mnist_conv_best.h5\n",
            "Epoch 33/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1981 - accuracy: 0.9455 - val_loss: 0.2112 - val_accuracy: 0.9421\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.93967 to 0.94208, saving model to mnist_conv_best.h5\n",
            "Epoch 34/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1938 - accuracy: 0.9459 - val_loss: 0.2086 - val_accuracy: 0.9424\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.94208 to 0.94242, saving model to mnist_conv_best.h5\n",
            "Epoch 35/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1890 - accuracy: 0.9479 - val_loss: 0.2038 - val_accuracy: 0.9436\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.94242 to 0.94358, saving model to mnist_conv_best.h5\n",
            "Epoch 36/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1849 - accuracy: 0.9488 - val_loss: 0.2010 - val_accuracy: 0.9444\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.94358 to 0.94442, saving model to mnist_conv_best.h5\n",
            "Epoch 37/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1805 - accuracy: 0.9498 - val_loss: 0.1971 - val_accuracy: 0.9447\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.94442 to 0.94467, saving model to mnist_conv_best.h5\n",
            "Epoch 38/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1763 - accuracy: 0.9509 - val_loss: 0.1912 - val_accuracy: 0.9464\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.94467 to 0.94642, saving model to mnist_conv_best.h5\n",
            "Epoch 39/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.1721 - accuracy: 0.9528 - val_loss: 0.1878 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00039: val_accuracy improved from 0.94642 to 0.94917, saving model to mnist_conv_best.h5\n",
            "Epoch 40/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1685 - accuracy: 0.9531 - val_loss: 0.1839 - val_accuracy: 0.9480\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.94917\n",
            "Epoch 41/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1644 - accuracy: 0.9541 - val_loss: 0.1827 - val_accuracy: 0.9501\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.94917 to 0.95008, saving model to mnist_conv_best.h5\n",
            "Epoch 42/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1606 - accuracy: 0.9558 - val_loss: 0.1762 - val_accuracy: 0.9532\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.95008 to 0.95325, saving model to mnist_conv_best.h5\n",
            "Epoch 43/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1570 - accuracy: 0.9568 - val_loss: 0.1723 - val_accuracy: 0.9525\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.95325\n",
            "Epoch 44/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1534 - accuracy: 0.9578 - val_loss: 0.1693 - val_accuracy: 0.9532\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.95325\n",
            "Epoch 45/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1500 - accuracy: 0.9589 - val_loss: 0.1661 - val_accuracy: 0.9548\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.95325 to 0.95483, saving model to mnist_conv_best.h5\n",
            "Epoch 46/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1469 - accuracy: 0.9601 - val_loss: 0.1632 - val_accuracy: 0.9553\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.95483 to 0.95533, saving model to mnist_conv_best.h5\n",
            "Epoch 47/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1439 - accuracy: 0.9613 - val_loss: 0.1619 - val_accuracy: 0.9557\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.95533 to 0.95575, saving model to mnist_conv_best.h5\n",
            "Epoch 48/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1408 - accuracy: 0.9618 - val_loss: 0.1581 - val_accuracy: 0.9565\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.95575 to 0.95650, saving model to mnist_conv_best.h5\n",
            "Epoch 49/10000\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 0.1379 - accuracy: 0.9628 - val_loss: 0.1559 - val_accuracy: 0.9582\n",
            "\n",
            "Epoch 00049: val_accuracy improved from 0.95650 to 0.95825, saving model to mnist_conv_best.h5\n",
            "Epoch 50/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1352 - accuracy: 0.9635 - val_loss: 0.1535 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.95825 to 0.95833, saving model to mnist_conv_best.h5\n",
            "Epoch 51/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1323 - accuracy: 0.9646 - val_loss: 0.1503 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.95833 to 0.95858, saving model to mnist_conv_best.h5\n",
            "Epoch 52/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1301 - accuracy: 0.9652 - val_loss: 0.1475 - val_accuracy: 0.9599\n",
            "\n",
            "Epoch 00052: val_accuracy improved from 0.95858 to 0.95992, saving model to mnist_conv_best.h5\n",
            "Epoch 53/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1273 - accuracy: 0.9655 - val_loss: 0.1449 - val_accuracy: 0.9604\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.95992 to 0.96042, saving model to mnist_conv_best.h5\n",
            "Epoch 54/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1252 - accuracy: 0.9661 - val_loss: 0.1427 - val_accuracy: 0.9622\n",
            "\n",
            "Epoch 00054: val_accuracy improved from 0.96042 to 0.96225, saving model to mnist_conv_best.h5\n",
            "Epoch 55/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.1227 - accuracy: 0.9667 - val_loss: 0.1400 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00055: val_accuracy improved from 0.96225 to 0.96233, saving model to mnist_conv_best.h5\n",
            "Epoch 56/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1205 - accuracy: 0.9674 - val_loss: 0.1387 - val_accuracy: 0.9628\n",
            "\n",
            "Epoch 00056: val_accuracy improved from 0.96233 to 0.96283, saving model to mnist_conv_best.h5\n",
            "Epoch 57/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1184 - accuracy: 0.9686 - val_loss: 0.1376 - val_accuracy: 0.9625\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.96283\n",
            "Epoch 58/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1163 - accuracy: 0.9689 - val_loss: 0.1337 - val_accuracy: 0.9635\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.96283 to 0.96350, saving model to mnist_conv_best.h5\n",
            "Epoch 59/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1145 - accuracy: 0.9695 - val_loss: 0.1327 - val_accuracy: 0.9639\n",
            "\n",
            "Epoch 00059: val_accuracy improved from 0.96350 to 0.96392, saving model to mnist_conv_best.h5\n",
            "Epoch 60/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1125 - accuracy: 0.9699 - val_loss: 0.1316 - val_accuracy: 0.9635\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.96392\n",
            "Epoch 61/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1106 - accuracy: 0.9705 - val_loss: 0.1298 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00061: val_accuracy improved from 0.96392 to 0.96425, saving model to mnist_conv_best.h5\n",
            "Epoch 62/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1091 - accuracy: 0.9714 - val_loss: 0.1291 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00062: val_accuracy improved from 0.96425 to 0.96475, saving model to mnist_conv_best.h5\n",
            "Epoch 63/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1072 - accuracy: 0.9717 - val_loss: 0.1272 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00063: val_accuracy improved from 0.96475 to 0.96492, saving model to mnist_conv_best.h5\n",
            "Epoch 64/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1058 - accuracy: 0.9723 - val_loss: 0.1243 - val_accuracy: 0.9651\n",
            "\n",
            "Epoch 00064: val_accuracy improved from 0.96492 to 0.96508, saving model to mnist_conv_best.h5\n",
            "Epoch 65/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1042 - accuracy: 0.9727 - val_loss: 0.1232 - val_accuracy: 0.9665\n",
            "\n",
            "Epoch 00065: val_accuracy improved from 0.96508 to 0.96650, saving model to mnist_conv_best.h5\n",
            "Epoch 66/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.1027 - accuracy: 0.9730 - val_loss: 0.1223 - val_accuracy: 0.9662\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.96650\n",
            "Epoch 67/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.1014 - accuracy: 0.9730 - val_loss: 0.1199 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.96650 to 0.96700, saving model to mnist_conv_best.h5\n",
            "Epoch 68/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0997 - accuracy: 0.9739 - val_loss: 0.1199 - val_accuracy: 0.9666\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.96700\n",
            "Epoch 69/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0985 - accuracy: 0.9737 - val_loss: 0.1185 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00069: val_accuracy improved from 0.96700 to 0.96733, saving model to mnist_conv_best.h5\n",
            "Epoch 70/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0972 - accuracy: 0.9743 - val_loss: 0.1193 - val_accuracy: 0.9667\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.96733\n",
            "Epoch 71/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0959 - accuracy: 0.9747 - val_loss: 0.1167 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00071: val_accuracy improved from 0.96733 to 0.96800, saving model to mnist_conv_best.h5\n",
            "Epoch 72/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0947 - accuracy: 0.9748 - val_loss: 0.1146 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.96800\n",
            "Epoch 73/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0934 - accuracy: 0.9752 - val_loss: 0.1139 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00073: val_accuracy improved from 0.96800 to 0.96817, saving model to mnist_conv_best.h5\n",
            "Epoch 74/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0922 - accuracy: 0.9756 - val_loss: 0.1125 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00074: val_accuracy improved from 0.96817 to 0.96825, saving model to mnist_conv_best.h5\n",
            "Epoch 75/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0912 - accuracy: 0.9759 - val_loss: 0.1118 - val_accuracy: 0.9684\n",
            "\n",
            "Epoch 00075: val_accuracy improved from 0.96825 to 0.96842, saving model to mnist_conv_best.h5\n",
            "Epoch 76/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0900 - accuracy: 0.9759 - val_loss: 0.1114 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.96842\n",
            "Epoch 77/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0891 - accuracy: 0.9762 - val_loss: 0.1101 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00077: val_accuracy improved from 0.96842 to 0.96858, saving model to mnist_conv_best.h5\n",
            "Epoch 78/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0880 - accuracy: 0.9768 - val_loss: 0.1096 - val_accuracy: 0.9694\n",
            "\n",
            "Epoch 00078: val_accuracy improved from 0.96858 to 0.96942, saving model to mnist_conv_best.h5\n",
            "Epoch 79/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0868 - accuracy: 0.9770 - val_loss: 0.1082 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00079: val_accuracy improved from 0.96942 to 0.96975, saving model to mnist_conv_best.h5\n",
            "Epoch 80/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0861 - accuracy: 0.9773 - val_loss: 0.1064 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00080: val_accuracy improved from 0.96975 to 0.97033, saving model to mnist_conv_best.h5\n",
            "Epoch 81/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0849 - accuracy: 0.9770 - val_loss: 0.1056 - val_accuracy: 0.9701\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.97033\n",
            "Epoch 82/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0842 - accuracy: 0.9777 - val_loss: 0.1054 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.97033\n",
            "Epoch 83/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0833 - accuracy: 0.9777 - val_loss: 0.1053 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.97033\n",
            "Epoch 84/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0823 - accuracy: 0.9783 - val_loss: 0.1044 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.97033\n",
            "Epoch 85/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0816 - accuracy: 0.9782 - val_loss: 0.1041 - val_accuracy: 0.9706\n",
            "\n",
            "Epoch 00085: val_accuracy improved from 0.97033 to 0.97058, saving model to mnist_conv_best.h5\n",
            "Epoch 86/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0807 - accuracy: 0.9786 - val_loss: 0.1013 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00086: val_accuracy improved from 0.97058 to 0.97067, saving model to mnist_conv_best.h5\n",
            "Epoch 87/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0798 - accuracy: 0.9789 - val_loss: 0.1013 - val_accuracy: 0.9713\n",
            "\n",
            "Epoch 00087: val_accuracy improved from 0.97067 to 0.97133, saving model to mnist_conv_best.h5\n",
            "Epoch 88/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0790 - accuracy: 0.9791 - val_loss: 0.1015 - val_accuracy: 0.9712\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.97133\n",
            "Epoch 89/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0783 - accuracy: 0.9792 - val_loss: 0.1008 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00089: val_accuracy improved from 0.97133 to 0.97217, saving model to mnist_conv_best.h5\n",
            "Epoch 90/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0775 - accuracy: 0.9794 - val_loss: 0.0997 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.97217\n",
            "Epoch 91/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0769 - accuracy: 0.9798 - val_loss: 0.0987 - val_accuracy: 0.9713\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.97217\n",
            "Epoch 92/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0761 - accuracy: 0.9800 - val_loss: 0.0989 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.97217\n",
            "Epoch 93/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0754 - accuracy: 0.9799 - val_loss: 0.0985 - val_accuracy: 0.9724\n",
            "\n",
            "Epoch 00093: val_accuracy improved from 0.97217 to 0.97242, saving model to mnist_conv_best.h5\n",
            "Epoch 94/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0748 - accuracy: 0.9801 - val_loss: 0.0964 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.97242\n",
            "Epoch 95/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0741 - accuracy: 0.9800 - val_loss: 0.0958 - val_accuracy: 0.9730\n",
            "\n",
            "Epoch 00095: val_accuracy improved from 0.97242 to 0.97300, saving model to mnist_conv_best.h5\n",
            "Epoch 96/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0734 - accuracy: 0.9806 - val_loss: 0.0961 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.97300\n",
            "Epoch 97/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0725 - accuracy: 0.9807 - val_loss: 0.0951 - val_accuracy: 0.9726\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.97300\n",
            "Epoch 98/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0721 - accuracy: 0.9812 - val_loss: 0.0939 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.97300\n",
            "Epoch 99/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0714 - accuracy: 0.9811 - val_loss: 0.0935 - val_accuracy: 0.9732\n",
            "\n",
            "Epoch 00099: val_accuracy improved from 0.97300 to 0.97317, saving model to mnist_conv_best.h5\n",
            "Epoch 100/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0707 - accuracy: 0.9812 - val_loss: 0.0944 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.97317\n",
            "Epoch 101/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0702 - accuracy: 0.9811 - val_loss: 0.0921 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00101: val_accuracy improved from 0.97317 to 0.97367, saving model to mnist_conv_best.h5\n",
            "Epoch 102/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0696 - accuracy: 0.9814 - val_loss: 0.0927 - val_accuracy: 0.9732\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.97367\n",
            "Epoch 103/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0690 - accuracy: 0.9818 - val_loss: 0.0927 - val_accuracy: 0.9728\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.97367\n",
            "Epoch 104/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0684 - accuracy: 0.9819 - val_loss: 0.0910 - val_accuracy: 0.9739\n",
            "\n",
            "Epoch 00104: val_accuracy improved from 0.97367 to 0.97392, saving model to mnist_conv_best.h5\n",
            "Epoch 105/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0679 - accuracy: 0.9818 - val_loss: 0.0917 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.97392\n",
            "Epoch 106/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0674 - accuracy: 0.9819 - val_loss: 0.0906 - val_accuracy: 0.9735\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.97392\n",
            "Epoch 107/10000\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 0.0667 - accuracy: 0.9821 - val_loss: 0.0901 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00107: val_accuracy improved from 0.97392 to 0.97458, saving model to mnist_conv_best.h5\n",
            "Epoch 108/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0663 - accuracy: 0.9821 - val_loss: 0.0897 - val_accuracy: 0.9743\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.97458\n",
            "Epoch 109/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0657 - accuracy: 0.9826 - val_loss: 0.0901 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.97458\n",
            "Epoch 110/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0653 - accuracy: 0.9828 - val_loss: 0.0892 - val_accuracy: 0.9738\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.97458\n",
            "Epoch 111/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0648 - accuracy: 0.9828 - val_loss: 0.0882 - val_accuracy: 0.9743\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.97458\n",
            "Epoch 112/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0643 - accuracy: 0.9830 - val_loss: 0.0921 - val_accuracy: 0.9718\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.97458\n",
            "Epoch 113/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0638 - accuracy: 0.9826 - val_loss: 0.0895 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.97458\n",
            "Epoch 114/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0634 - accuracy: 0.9831 - val_loss: 0.0863 - val_accuracy: 0.9754\n",
            "\n",
            "Epoch 00114: val_accuracy improved from 0.97458 to 0.97542, saving model to mnist_conv_best.h5\n",
            "Epoch 115/10000\n",
            "188/188 [==============================] - 15s 77ms/step - loss: 0.0628 - accuracy: 0.9835 - val_loss: 0.0882 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.97542\n",
            "Epoch 116/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0623 - accuracy: 0.9834 - val_loss: 0.0860 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.97542\n",
            "Epoch 117/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0616 - accuracy: 0.9837 - val_loss: 0.0872 - val_accuracy: 0.9747\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.97542\n",
            "Epoch 118/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0616 - accuracy: 0.9836 - val_loss: 0.0867 - val_accuracy: 0.9745\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.97542\n",
            "Epoch 119/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0612 - accuracy: 0.9837 - val_loss: 0.0853 - val_accuracy: 0.9747\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.97542\n",
            "Epoch 120/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0606 - accuracy: 0.9837 - val_loss: 0.0852 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.97542\n",
            "Epoch 121/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0602 - accuracy: 0.9843 - val_loss: 0.0843 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.97542\n",
            "Epoch 122/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0598 - accuracy: 0.9837 - val_loss: 0.0842 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.97542\n",
            "Epoch 123/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0594 - accuracy: 0.9843 - val_loss: 0.0839 - val_accuracy: 0.9755\n",
            "\n",
            "Epoch 00123: val_accuracy improved from 0.97542 to 0.97550, saving model to mnist_conv_best.h5\n",
            "Epoch 124/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0590 - accuracy: 0.9841 - val_loss: 0.0836 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00124: val_accuracy improved from 0.97550 to 0.97558, saving model to mnist_conv_best.h5\n",
            "Epoch 125/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0585 - accuracy: 0.9844 - val_loss: 0.0832 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00125: val_accuracy improved from 0.97558 to 0.97608, saving model to mnist_conv_best.h5\n",
            "Epoch 126/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0582 - accuracy: 0.9847 - val_loss: 0.0830 - val_accuracy: 0.9758\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.97608\n",
            "Epoch 127/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0578 - accuracy: 0.9844 - val_loss: 0.0832 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.97608\n",
            "Epoch 128/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0574 - accuracy: 0.9848 - val_loss: 0.0823 - val_accuracy: 0.9764\n",
            "\n",
            "Epoch 00128: val_accuracy improved from 0.97608 to 0.97642, saving model to mnist_conv_best.h5\n",
            "Epoch 129/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0570 - accuracy: 0.9850 - val_loss: 0.0842 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.97642\n",
            "Epoch 130/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0567 - accuracy: 0.9848 - val_loss: 0.0832 - val_accuracy: 0.9755\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.97642\n",
            "Epoch 131/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0563 - accuracy: 0.9851 - val_loss: 0.0816 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.97642\n",
            "Epoch 132/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0559 - accuracy: 0.9852 - val_loss: 0.0819 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.97642\n",
            "Epoch 133/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0555 - accuracy: 0.9855 - val_loss: 0.0828 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.97642\n",
            "Epoch 134/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0553 - accuracy: 0.9854 - val_loss: 0.0807 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00134: val_accuracy improved from 0.97642 to 0.97683, saving model to mnist_conv_best.h5\n",
            "Epoch 135/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0548 - accuracy: 0.9854 - val_loss: 0.0806 - val_accuracy: 0.9760\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.97683\n",
            "Epoch 136/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0546 - accuracy: 0.9853 - val_loss: 0.0804 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.97683\n",
            "Epoch 137/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0543 - accuracy: 0.9855 - val_loss: 0.0805 - val_accuracy: 0.9758\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.97683\n",
            "Epoch 138/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0538 - accuracy: 0.9858 - val_loss: 0.0803 - val_accuracy: 0.9761\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.97683\n",
            "Epoch 139/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0535 - accuracy: 0.9857 - val_loss: 0.0800 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.97683\n",
            "Epoch 140/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0531 - accuracy: 0.9860 - val_loss: 0.0799 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.97683\n",
            "Epoch 141/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0530 - accuracy: 0.9859 - val_loss: 0.0796 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.97683\n",
            "Epoch 142/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0526 - accuracy: 0.9858 - val_loss: 0.0794 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.97683\n",
            "Epoch 143/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0520 - accuracy: 0.9865 - val_loss: 0.0790 - val_accuracy: 0.9769\n",
            "\n",
            "Epoch 00143: val_accuracy improved from 0.97683 to 0.97692, saving model to mnist_conv_best.h5\n",
            "Epoch 144/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0519 - accuracy: 0.9864 - val_loss: 0.0787 - val_accuracy: 0.9767\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.97692\n",
            "Epoch 145/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0516 - accuracy: 0.9864 - val_loss: 0.0782 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.97692\n",
            "Epoch 146/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0513 - accuracy: 0.9861 - val_loss: 0.0780 - val_accuracy: 0.9767\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.97692\n",
            "Epoch 147/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0511 - accuracy: 0.9865 - val_loss: 0.0776 - val_accuracy: 0.9764\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.97692\n",
            "Epoch 148/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0507 - accuracy: 0.9865 - val_loss: 0.0774 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00148: val_accuracy improved from 0.97692 to 0.97733, saving model to mnist_conv_best.h5\n",
            "Epoch 149/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0505 - accuracy: 0.9866 - val_loss: 0.0774 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.97733\n",
            "Epoch 150/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0500 - accuracy: 0.9868 - val_loss: 0.0781 - val_accuracy: 0.9770\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.97733\n",
            "Epoch 151/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0498 - accuracy: 0.9868 - val_loss: 0.0767 - val_accuracy: 0.9776\n",
            "\n",
            "Epoch 00151: val_accuracy improved from 0.97733 to 0.97758, saving model to mnist_conv_best.h5\n",
            "Epoch 152/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0496 - accuracy: 0.9871 - val_loss: 0.0765 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00152: val_accuracy improved from 0.97758 to 0.97800, saving model to mnist_conv_best.h5\n",
            "Epoch 153/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0492 - accuracy: 0.9869 - val_loss: 0.0770 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.97800\n",
            "Epoch 154/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0490 - accuracy: 0.9870 - val_loss: 0.0767 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.97800\n",
            "Epoch 155/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0487 - accuracy: 0.9870 - val_loss: 0.0766 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.97800\n",
            "Epoch 156/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0485 - accuracy: 0.9874 - val_loss: 0.0762 - val_accuracy: 0.9771\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.97800\n",
            "Epoch 157/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0481 - accuracy: 0.9875 - val_loss: 0.0763 - val_accuracy: 0.9776\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.97800\n",
            "Epoch 158/10000\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 0.0480 - accuracy: 0.9873 - val_loss: 0.0766 - val_accuracy: 0.9776\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.97800\n",
            "Epoch 159/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0477 - accuracy: 0.9876 - val_loss: 0.0776 - val_accuracy: 0.9767\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.97800\n",
            "Epoch 160/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0475 - accuracy: 0.9877 - val_loss: 0.0752 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.97800\n",
            "Epoch 161/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0471 - accuracy: 0.9876 - val_loss: 0.0758 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.97800\n",
            "Epoch 162/10000\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 0.0469 - accuracy: 0.9876 - val_loss: 0.0751 - val_accuracy: 0.9779\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.97800\n",
            "Epoch 00162: early stopping\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0547 - accuracy: 0.9852\n",
            "Accuracy for the training set: 0.9852166771888733\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0630 - accuracy: 0.9812\n",
            "Accuracy for the testing set: 0.9811999797821045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution (Conv2D)         (None, 28, 28, 16)        416       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                125450    \n",
            "=================================================================\n",
            "Total params: 125,866\n",
            "Trainable params: 125,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "188/188 [==============================] - 19s 98ms/step - loss: 1.0395 - accuracy: 0.7587 - val_loss: 0.4747 - val_accuracy: 0.8721\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.87208, saving model to mnist_conv_best.h5\n",
            "Epoch 2/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.4159 - accuracy: 0.8843 - val_loss: 0.3839 - val_accuracy: 0.8917\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.87208 to 0.89175, saving model to mnist_conv_best.h5\n",
            "Epoch 3/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.3641 - accuracy: 0.8958 - val_loss: 0.3579 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89175 to 0.89825, saving model to mnist_conv_best.h5\n",
            "Epoch 4/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.3419 - accuracy: 0.9014 - val_loss: 0.3461 - val_accuracy: 0.9017\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89825 to 0.90167, saving model to mnist_conv_best.h5\n",
            "Epoch 5/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.3274 - accuracy: 0.9061 - val_loss: 0.3290 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90167 to 0.90733, saving model to mnist_conv_best.h5\n",
            "Epoch 6/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.3169 - accuracy: 0.9086 - val_loss: 0.3254 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90733 to 0.90817, saving model to mnist_conv_best.h5\n",
            "Epoch 7/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.3085 - accuracy: 0.9105 - val_loss: 0.3160 - val_accuracy: 0.9117\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90817 to 0.91167, saving model to mnist_conv_best.h5\n",
            "Epoch 8/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.3020 - accuracy: 0.9120 - val_loss: 0.3060 - val_accuracy: 0.9127\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.91167 to 0.91267, saving model to mnist_conv_best.h5\n",
            "Epoch 9/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.2953 - accuracy: 0.9144 - val_loss: 0.3071 - val_accuracy: 0.9137\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.91267 to 0.91367, saving model to mnist_conv_best.h5\n",
            "Epoch 10/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.2893 - accuracy: 0.9174 - val_loss: 0.3106 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.91367\n",
            "Epoch 11/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2839 - accuracy: 0.9176 - val_loss: 0.2999 - val_accuracy: 0.9143\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.91367 to 0.91425, saving model to mnist_conv_best.h5\n",
            "Epoch 12/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.2793 - accuracy: 0.9192 - val_loss: 0.2880 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.91425 to 0.92033, saving model to mnist_conv_best.h5\n",
            "Epoch 13/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.2737 - accuracy: 0.9220 - val_loss: 0.2874 - val_accuracy: 0.9199\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.92033\n",
            "Epoch 14/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.2682 - accuracy: 0.9233 - val_loss: 0.2768 - val_accuracy: 0.9243\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.92033 to 0.92433, saving model to mnist_conv_best.h5\n",
            "Epoch 15/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2631 - accuracy: 0.9250 - val_loss: 0.2751 - val_accuracy: 0.9228\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.92433\n",
            "Epoch 16/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.2576 - accuracy: 0.9269 - val_loss: 0.2678 - val_accuracy: 0.9257\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.92433 to 0.92567, saving model to mnist_conv_best.h5\n",
            "Epoch 17/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2522 - accuracy: 0.9284 - val_loss: 0.2701 - val_accuracy: 0.9235\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.92567\n",
            "Epoch 18/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.2468 - accuracy: 0.9302 - val_loss: 0.2588 - val_accuracy: 0.9280\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.92567 to 0.92800, saving model to mnist_conv_best.h5\n",
            "Epoch 19/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2410 - accuracy: 0.9324 - val_loss: 0.2539 - val_accuracy: 0.9299\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.92800 to 0.92992, saving model to mnist_conv_best.h5\n",
            "Epoch 20/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2345 - accuracy: 0.9336 - val_loss: 0.2572 - val_accuracy: 0.9278\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.92992\n",
            "Epoch 21/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2294 - accuracy: 0.9354 - val_loss: 0.2434 - val_accuracy: 0.9319\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.92992 to 0.93192, saving model to mnist_conv_best.h5\n",
            "Epoch 22/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2230 - accuracy: 0.9369 - val_loss: 0.2351 - val_accuracy: 0.9342\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.93192 to 0.93417, saving model to mnist_conv_best.h5\n",
            "Epoch 23/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2169 - accuracy: 0.9385 - val_loss: 0.2308 - val_accuracy: 0.9374\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.93417 to 0.93742, saving model to mnist_conv_best.h5\n",
            "Epoch 24/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2112 - accuracy: 0.9405 - val_loss: 0.2296 - val_accuracy: 0.9356\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93742\n",
            "Epoch 25/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.2045 - accuracy: 0.9424 - val_loss: 0.2197 - val_accuracy: 0.9385\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.93742 to 0.93850, saving model to mnist_conv_best.h5\n",
            "Epoch 26/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1990 - accuracy: 0.9444 - val_loss: 0.2172 - val_accuracy: 0.9402\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.93850 to 0.94017, saving model to mnist_conv_best.h5\n",
            "Epoch 27/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1933 - accuracy: 0.9459 - val_loss: 0.2068 - val_accuracy: 0.9430\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.94017 to 0.94300, saving model to mnist_conv_best.h5\n",
            "Epoch 28/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1876 - accuracy: 0.9475 - val_loss: 0.2025 - val_accuracy: 0.9435\n",
            "\n",
            "Epoch 00028: val_accuracy improved from 0.94300 to 0.94350, saving model to mnist_conv_best.h5\n",
            "Epoch 29/10000\n",
            "188/188 [==============================] - 18s 97ms/step - loss: 0.1816 - accuracy: 0.9495 - val_loss: 0.1944 - val_accuracy: 0.9463\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.94350 to 0.94625, saving model to mnist_conv_best.h5\n",
            "Epoch 30/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1765 - accuracy: 0.9506 - val_loss: 0.1920 - val_accuracy: 0.9465\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.94625 to 0.94650, saving model to mnist_conv_best.h5\n",
            "Epoch 31/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1713 - accuracy: 0.9522 - val_loss: 0.1835 - val_accuracy: 0.9491\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.94650 to 0.94908, saving model to mnist_conv_best.h5\n",
            "Epoch 32/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.1663 - accuracy: 0.9541 - val_loss: 0.1795 - val_accuracy: 0.9500\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.94908 to 0.95000, saving model to mnist_conv_best.h5\n",
            "Epoch 33/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1615 - accuracy: 0.9551 - val_loss: 0.1781 - val_accuracy: 0.9499\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.95000\n",
            "Epoch 34/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1567 - accuracy: 0.9564 - val_loss: 0.1713 - val_accuracy: 0.9517\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.95000 to 0.95167, saving model to mnist_conv_best.h5\n",
            "Epoch 35/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1521 - accuracy: 0.9583 - val_loss: 0.1674 - val_accuracy: 0.9532\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.95167 to 0.95325, saving model to mnist_conv_best.h5\n",
            "Epoch 36/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1478 - accuracy: 0.9586 - val_loss: 0.1616 - val_accuracy: 0.9548\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.95325 to 0.95483, saving model to mnist_conv_best.h5\n",
            "Epoch 37/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1441 - accuracy: 0.9603 - val_loss: 0.1580 - val_accuracy: 0.9559\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.95483 to 0.95592, saving model to mnist_conv_best.h5\n",
            "Epoch 38/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1401 - accuracy: 0.9612 - val_loss: 0.1543 - val_accuracy: 0.9568\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.95592 to 0.95683, saving model to mnist_conv_best.h5\n",
            "Epoch 39/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1362 - accuracy: 0.9628 - val_loss: 0.1526 - val_accuracy: 0.9566\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.95683\n",
            "Epoch 40/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1330 - accuracy: 0.9631 - val_loss: 0.1479 - val_accuracy: 0.9586\n",
            "\n",
            "Epoch 00040: val_accuracy improved from 0.95683 to 0.95858, saving model to mnist_conv_best.h5\n",
            "Epoch 41/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1298 - accuracy: 0.9641 - val_loss: 0.1431 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.95858 to 0.96000, saving model to mnist_conv_best.h5\n",
            "Epoch 42/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1265 - accuracy: 0.9654 - val_loss: 0.1433 - val_accuracy: 0.9605\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.96000 to 0.96050, saving model to mnist_conv_best.h5\n",
            "Epoch 43/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1235 - accuracy: 0.9659 - val_loss: 0.1371 - val_accuracy: 0.9614\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.96050 to 0.96142, saving model to mnist_conv_best.h5\n",
            "Epoch 44/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1204 - accuracy: 0.9672 - val_loss: 0.1353 - val_accuracy: 0.9622\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.96142 to 0.96217, saving model to mnist_conv_best.h5\n",
            "Epoch 45/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1177 - accuracy: 0.9674 - val_loss: 0.1326 - val_accuracy: 0.9630\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.96217 to 0.96300, saving model to mnist_conv_best.h5\n",
            "Epoch 46/10000\n",
            "188/188 [==============================] - 19s 98ms/step - loss: 0.1149 - accuracy: 0.9687 - val_loss: 0.1318 - val_accuracy: 0.9637\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.96300 to 0.96367, saving model to mnist_conv_best.h5\n",
            "Epoch 47/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1128 - accuracy: 0.9688 - val_loss: 0.1275 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.96367 to 0.96475, saving model to mnist_conv_best.h5\n",
            "Epoch 48/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1103 - accuracy: 0.9700 - val_loss: 0.1261 - val_accuracy: 0.9649\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.96475 to 0.96492, saving model to mnist_conv_best.h5\n",
            "Epoch 49/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1080 - accuracy: 0.9710 - val_loss: 0.1241 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00049: val_accuracy improved from 0.96492 to 0.96592, saving model to mnist_conv_best.h5\n",
            "Epoch 50/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1057 - accuracy: 0.9712 - val_loss: 0.1221 - val_accuracy: 0.9657\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.96592\n",
            "Epoch 51/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1039 - accuracy: 0.9714 - val_loss: 0.1189 - val_accuracy: 0.9662\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.96592 to 0.96617, saving model to mnist_conv_best.h5\n",
            "Epoch 52/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1020 - accuracy: 0.9726 - val_loss: 0.1210 - val_accuracy: 0.9662\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.96617\n",
            "Epoch 53/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.1001 - accuracy: 0.9730 - val_loss: 0.1161 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.96617 to 0.96775, saving model to mnist_conv_best.h5\n",
            "Epoch 54/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0984 - accuracy: 0.9738 - val_loss: 0.1155 - val_accuracy: 0.9675\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.96775\n",
            "Epoch 55/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0965 - accuracy: 0.9740 - val_loss: 0.1121 - val_accuracy: 0.9686\n",
            "\n",
            "Epoch 00055: val_accuracy improved from 0.96775 to 0.96858, saving model to mnist_conv_best.h5\n",
            "Epoch 56/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0949 - accuracy: 0.9744 - val_loss: 0.1112 - val_accuracy: 0.9683\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.96858\n",
            "Epoch 57/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0933 - accuracy: 0.9747 - val_loss: 0.1122 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.96858\n",
            "Epoch 58/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0919 - accuracy: 0.9754 - val_loss: 0.1083 - val_accuracy: 0.9700\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.96858 to 0.97000, saving model to mnist_conv_best.h5\n",
            "Epoch 59/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0905 - accuracy: 0.9756 - val_loss: 0.1070 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.97000\n",
            "Epoch 60/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0888 - accuracy: 0.9762 - val_loss: 0.1055 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00060: val_accuracy improved from 0.97000 to 0.97025, saving model to mnist_conv_best.h5\n",
            "Epoch 61/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0877 - accuracy: 0.9761 - val_loss: 0.1043 - val_accuracy: 0.9709\n",
            "\n",
            "Epoch 00061: val_accuracy improved from 0.97025 to 0.97092, saving model to mnist_conv_best.h5\n",
            "Epoch 62/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0863 - accuracy: 0.9767 - val_loss: 0.1027 - val_accuracy: 0.9718\n",
            "\n",
            "Epoch 00062: val_accuracy improved from 0.97092 to 0.97175, saving model to mnist_conv_best.h5\n",
            "Epoch 63/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0850 - accuracy: 0.9772 - val_loss: 0.1024 - val_accuracy: 0.9715\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.97175\n",
            "Epoch 64/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0841 - accuracy: 0.9771 - val_loss: 0.1008 - val_accuracy: 0.9714\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.97175\n",
            "Epoch 65/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0828 - accuracy: 0.9776 - val_loss: 0.1005 - val_accuracy: 0.9718\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.97175\n",
            "Epoch 66/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0816 - accuracy: 0.9780 - val_loss: 0.1001 - val_accuracy: 0.9714\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.97175\n",
            "Epoch 67/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0806 - accuracy: 0.9781 - val_loss: 0.0987 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.97175 to 0.97208, saving model to mnist_conv_best.h5\n",
            "Epoch 68/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0795 - accuracy: 0.9784 - val_loss: 0.0973 - val_accuracy: 0.9729\n",
            "\n",
            "Epoch 00068: val_accuracy improved from 0.97208 to 0.97292, saving model to mnist_conv_best.h5\n",
            "Epoch 69/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0784 - accuracy: 0.9790 - val_loss: 0.0968 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.97292\n",
            "Epoch 70/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0776 - accuracy: 0.9788 - val_loss: 0.0951 - val_accuracy: 0.9728\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.97292\n",
            "Epoch 71/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0766 - accuracy: 0.9792 - val_loss: 0.0958 - val_accuracy: 0.9724\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.97292\n",
            "Epoch 72/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0758 - accuracy: 0.9795 - val_loss: 0.0953 - val_accuracy: 0.9728\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.97292\n",
            "Epoch 73/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0746 - accuracy: 0.9798 - val_loss: 0.0944 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.97292\n",
            "Epoch 74/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0738 - accuracy: 0.9801 - val_loss: 0.0924 - val_accuracy: 0.9744\n",
            "\n",
            "Epoch 00074: val_accuracy improved from 0.97292 to 0.97442, saving model to mnist_conv_best.h5\n",
            "Epoch 75/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0730 - accuracy: 0.9800 - val_loss: 0.0919 - val_accuracy: 0.9741\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.97442\n",
            "Epoch 76/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0722 - accuracy: 0.9802 - val_loss: 0.0910 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.97442\n",
            "Epoch 77/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0713 - accuracy: 0.9805 - val_loss: 0.0895 - val_accuracy: 0.9747\n",
            "\n",
            "Epoch 00077: val_accuracy improved from 0.97442 to 0.97475, saving model to mnist_conv_best.h5\n",
            "Epoch 78/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0706 - accuracy: 0.9809 - val_loss: 0.0917 - val_accuracy: 0.9739\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.97475\n",
            "Epoch 79/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0699 - accuracy: 0.9809 - val_loss: 0.0890 - val_accuracy: 0.9750\n",
            "\n",
            "Epoch 00079: val_accuracy improved from 0.97475 to 0.97500, saving model to mnist_conv_best.h5\n",
            "Epoch 80/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0692 - accuracy: 0.9816 - val_loss: 0.0879 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00080: val_accuracy improved from 0.97500 to 0.97525, saving model to mnist_conv_best.h5\n",
            "Epoch 81/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0685 - accuracy: 0.9816 - val_loss: 0.0868 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.97525\n",
            "Epoch 82/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0678 - accuracy: 0.9822 - val_loss: 0.0862 - val_accuracy: 0.9755\n",
            "\n",
            "Epoch 00082: val_accuracy improved from 0.97525 to 0.97550, saving model to mnist_conv_best.h5\n",
            "Epoch 83/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0671 - accuracy: 0.9819 - val_loss: 0.0867 - val_accuracy: 0.9745\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.97550\n",
            "Epoch 84/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0665 - accuracy: 0.9824 - val_loss: 0.0850 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00084: val_accuracy improved from 0.97550 to 0.97558, saving model to mnist_conv_best.h5\n",
            "Epoch 85/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0658 - accuracy: 0.9822 - val_loss: 0.0864 - val_accuracy: 0.9754\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.97558\n",
            "Epoch 86/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0652 - accuracy: 0.9827 - val_loss: 0.0844 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.97558\n",
            "Epoch 87/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0646 - accuracy: 0.9828 - val_loss: 0.0840 - val_accuracy: 0.9757\n",
            "\n",
            "Epoch 00087: val_accuracy improved from 0.97558 to 0.97567, saving model to mnist_conv_best.h5\n",
            "Epoch 88/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0639 - accuracy: 0.9829 - val_loss: 0.0843 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.97567\n",
            "Epoch 89/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0634 - accuracy: 0.9831 - val_loss: 0.0823 - val_accuracy: 0.9765\n",
            "\n",
            "Epoch 00089: val_accuracy improved from 0.97567 to 0.97650, saving model to mnist_conv_best.h5\n",
            "Epoch 90/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0627 - accuracy: 0.9834 - val_loss: 0.0827 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00090: val_accuracy improved from 0.97650 to 0.97675, saving model to mnist_conv_best.h5\n",
            "Epoch 91/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0622 - accuracy: 0.9836 - val_loss: 0.0818 - val_accuracy: 0.9769\n",
            "\n",
            "Epoch 00091: val_accuracy improved from 0.97675 to 0.97692, saving model to mnist_conv_best.h5\n",
            "Epoch 92/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0616 - accuracy: 0.9836 - val_loss: 0.0815 - val_accuracy: 0.9765\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.97692\n",
            "Epoch 93/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0611 - accuracy: 0.9838 - val_loss: 0.0813 - val_accuracy: 0.9763\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.97692\n",
            "Epoch 94/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0604 - accuracy: 0.9837 - val_loss: 0.0818 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.97692\n",
            "Epoch 95/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0601 - accuracy: 0.9841 - val_loss: 0.0801 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.97692\n",
            "Epoch 96/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0596 - accuracy: 0.9842 - val_loss: 0.0804 - val_accuracy: 0.9769\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.97692\n",
            "Epoch 97/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0590 - accuracy: 0.9841 - val_loss: 0.0802 - val_accuracy: 0.9769\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.97692\n",
            "Epoch 98/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0585 - accuracy: 0.9843 - val_loss: 0.0796 - val_accuracy: 0.9778\n",
            "\n",
            "Epoch 00098: val_accuracy improved from 0.97692 to 0.97783, saving model to mnist_conv_best.h5\n",
            "Epoch 99/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0581 - accuracy: 0.9847 - val_loss: 0.0786 - val_accuracy: 0.9779\n",
            "\n",
            "Epoch 00099: val_accuracy improved from 0.97783 to 0.97792, saving model to mnist_conv_best.h5\n",
            "Epoch 100/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0577 - accuracy: 0.9847 - val_loss: 0.0805 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.97792\n",
            "Epoch 101/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0572 - accuracy: 0.9847 - val_loss: 0.0784 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.97792\n",
            "Epoch 102/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0568 - accuracy: 0.9849 - val_loss: 0.0793 - val_accuracy: 0.9765\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.97792\n",
            "Epoch 103/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0562 - accuracy: 0.9847 - val_loss: 0.0777 - val_accuracy: 0.9776\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.97792\n",
            "Epoch 104/10000\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0558 - accuracy: 0.9852 - val_loss: 0.0771 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.97792\n",
            "Epoch 105/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0553 - accuracy: 0.9854 - val_loss: 0.0780 - val_accuracy: 0.9774\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.97792\n",
            "Epoch 106/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0550 - accuracy: 0.9855 - val_loss: 0.0768 - val_accuracy: 0.9774\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.97792\n",
            "Epoch 107/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0546 - accuracy: 0.9858 - val_loss: 0.0772 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00107: val_accuracy improved from 0.97792 to 0.97817, saving model to mnist_conv_best.h5\n",
            "Epoch 108/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0542 - accuracy: 0.9856 - val_loss: 0.0759 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00108: val_accuracy improved from 0.97817 to 0.97850, saving model to mnist_conv_best.h5\n",
            "Epoch 109/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0537 - accuracy: 0.9858 - val_loss: 0.0758 - val_accuracy: 0.9783\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.97850\n",
            "Epoch 110/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0534 - accuracy: 0.9856 - val_loss: 0.0753 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.97850\n",
            "Epoch 111/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0529 - accuracy: 0.9858 - val_loss: 0.0752 - val_accuracy: 0.9783\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.97850\n",
            "Epoch 112/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0526 - accuracy: 0.9862 - val_loss: 0.0751 - val_accuracy: 0.9786\n",
            "\n",
            "Epoch 00112: val_accuracy improved from 0.97850 to 0.97858, saving model to mnist_conv_best.h5\n",
            "Epoch 113/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0523 - accuracy: 0.9860 - val_loss: 0.0751 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.97858\n",
            "Epoch 114/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0519 - accuracy: 0.9863 - val_loss: 0.0746 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00114: val_accuracy improved from 0.97858 to 0.97917, saving model to mnist_conv_best.h5\n",
            "Epoch 115/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0516 - accuracy: 0.9860 - val_loss: 0.0740 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.97917\n",
            "Epoch 116/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0512 - accuracy: 0.9865 - val_loss: 0.0749 - val_accuracy: 0.9786\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.97917\n",
            "Epoch 117/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0508 - accuracy: 0.9870 - val_loss: 0.0745 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00117: val_accuracy improved from 0.97917 to 0.97942, saving model to mnist_conv_best.h5\n",
            "Epoch 118/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0504 - accuracy: 0.9868 - val_loss: 0.0733 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.97942\n",
            "Epoch 119/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0502 - accuracy: 0.9867 - val_loss: 0.0732 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00119: val_accuracy improved from 0.97942 to 0.97950, saving model to mnist_conv_best.h5\n",
            "Epoch 120/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0498 - accuracy: 0.9869 - val_loss: 0.0727 - val_accuracy: 0.9791\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.97950\n",
            "Epoch 121/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0495 - accuracy: 0.9867 - val_loss: 0.0726 - val_accuracy: 0.9793\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.97950\n",
            "Epoch 122/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0491 - accuracy: 0.9871 - val_loss: 0.0737 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.97950\n",
            "Epoch 123/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0488 - accuracy: 0.9870 - val_loss: 0.0726 - val_accuracy: 0.9793\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.97950\n",
            "Epoch 124/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0486 - accuracy: 0.9871 - val_loss: 0.0720 - val_accuracy: 0.9791\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.97950\n",
            "Epoch 125/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0482 - accuracy: 0.9872 - val_loss: 0.0723 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.97950\n",
            "Epoch 126/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0480 - accuracy: 0.9875 - val_loss: 0.0722 - val_accuracy: 0.9791\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.97950\n",
            "Epoch 127/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0475 - accuracy: 0.9873 - val_loss: 0.0717 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.97950\n",
            "Epoch 128/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0473 - accuracy: 0.9875 - val_loss: 0.0724 - val_accuracy: 0.9789\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.97950\n",
            "Epoch 129/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0471 - accuracy: 0.9874 - val_loss: 0.0711 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00129: val_accuracy improved from 0.97950 to 0.97967, saving model to mnist_conv_best.h5\n",
            "Epoch 130/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0466 - accuracy: 0.9879 - val_loss: 0.0718 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.97967\n",
            "Epoch 131/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0464 - accuracy: 0.9878 - val_loss: 0.0719 - val_accuracy: 0.9788\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.97967\n",
            "Epoch 132/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0462 - accuracy: 0.9878 - val_loss: 0.0711 - val_accuracy: 0.9790\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.97967\n",
            "Epoch 133/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0458 - accuracy: 0.9879 - val_loss: 0.0709 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.97967\n",
            "Epoch 134/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0457 - accuracy: 0.9881 - val_loss: 0.0700 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00134: val_accuracy improved from 0.97967 to 0.98008, saving model to mnist_conv_best.h5\n",
            "Epoch 135/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0453 - accuracy: 0.9881 - val_loss: 0.0704 - val_accuracy: 0.9803\n",
            "\n",
            "Epoch 00135: val_accuracy improved from 0.98008 to 0.98025, saving model to mnist_conv_best.h5\n",
            "Epoch 136/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0450 - accuracy: 0.9885 - val_loss: 0.0697 - val_accuracy: 0.9804\n",
            "\n",
            "Epoch 00136: val_accuracy improved from 0.98025 to 0.98042, saving model to mnist_conv_best.h5\n",
            "Epoch 137/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0447 - accuracy: 0.9880 - val_loss: 0.0708 - val_accuracy: 0.9799\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.98042\n",
            "Epoch 138/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0445 - accuracy: 0.9884 - val_loss: 0.0701 - val_accuracy: 0.9803\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.98042\n",
            "Epoch 139/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0442 - accuracy: 0.9884 - val_loss: 0.0697 - val_accuracy: 0.9796\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.98042\n",
            "Epoch 140/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0440 - accuracy: 0.9883 - val_loss: 0.0694 - val_accuracy: 0.9804\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.98042\n",
            "Epoch 141/10000\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 0.0438 - accuracy: 0.9884 - val_loss: 0.0700 - val_accuracy: 0.9803\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.98042\n",
            "Epoch 142/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0436 - accuracy: 0.9886 - val_loss: 0.0695 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.98042\n",
            "Epoch 143/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0433 - accuracy: 0.9887 - val_loss: 0.0693 - val_accuracy: 0.9793\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.98042\n",
            "Epoch 144/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0430 - accuracy: 0.9886 - val_loss: 0.0705 - val_accuracy: 0.9802\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.98042\n",
            "Epoch 145/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0428 - accuracy: 0.9885 - val_loss: 0.0700 - val_accuracy: 0.9800\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.98042\n",
            "Epoch 146/10000\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0426 - accuracy: 0.9889 - val_loss: 0.0685 - val_accuracy: 0.9797\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.98042\n",
            "Epoch 00146: early stopping\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0494 - accuracy: 0.9868\n",
            "Accuracy for the training set: 0.9867500066757202\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0558 - accuracy: 0.9823\n",
            "Accuracy for the testing set: 0.9822999835014343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution (Conv2D)         (None, 28, 28, 16)        592       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                125450    \n",
            "=================================================================\n",
            "Total params: 126,042\n",
            "Trainable params: 126,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "188/188 [==============================] - 24s 124ms/step - loss: 1.0294 - accuracy: 0.7653 - val_loss: 0.4698 - val_accuracy: 0.8741\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.87408, saving model to mnist_conv_best.h5\n",
            "Epoch 2/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.4163 - accuracy: 0.8841 - val_loss: 0.3858 - val_accuracy: 0.8926\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.87408 to 0.89258, saving model to mnist_conv_best.h5\n",
            "Epoch 3/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.3683 - accuracy: 0.8957 - val_loss: 0.3652 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89258 to 0.89417, saving model to mnist_conv_best.h5\n",
            "Epoch 4/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.3467 - accuracy: 0.9005 - val_loss: 0.3454 - val_accuracy: 0.9025\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89417 to 0.90250, saving model to mnist_conv_best.h5\n",
            "Epoch 5/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.3316 - accuracy: 0.9050 - val_loss: 0.3324 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90250 to 0.90683, saving model to mnist_conv_best.h5\n",
            "Epoch 6/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.3204 - accuracy: 0.9078 - val_loss: 0.3206 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90683 to 0.90900, saving model to mnist_conv_best.h5\n",
            "Epoch 7/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.3101 - accuracy: 0.9107 - val_loss: 0.3136 - val_accuracy: 0.9125\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90900 to 0.91250, saving model to mnist_conv_best.h5\n",
            "Epoch 8/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.3020 - accuracy: 0.9130 - val_loss: 0.3083 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.91250 to 0.91500, saving model to mnist_conv_best.h5\n",
            "Epoch 9/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2935 - accuracy: 0.9149 - val_loss: 0.3107 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.91500\n",
            "Epoch 10/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2861 - accuracy: 0.9174 - val_loss: 0.2951 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.91500 to 0.91717, saving model to mnist_conv_best.h5\n",
            "Epoch 11/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2784 - accuracy: 0.9202 - val_loss: 0.2947 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.91717 to 0.91767, saving model to mnist_conv_best.h5\n",
            "Epoch 12/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2708 - accuracy: 0.9224 - val_loss: 0.2759 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.91767 to 0.92358, saving model to mnist_conv_best.h5\n",
            "Epoch 13/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2632 - accuracy: 0.9250 - val_loss: 0.2729 - val_accuracy: 0.9243\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.92358 to 0.92433, saving model to mnist_conv_best.h5\n",
            "Epoch 14/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2547 - accuracy: 0.9276 - val_loss: 0.2612 - val_accuracy: 0.9284\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.92433 to 0.92842, saving model to mnist_conv_best.h5\n",
            "Epoch 15/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2463 - accuracy: 0.9299 - val_loss: 0.2546 - val_accuracy: 0.9298\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.92842 to 0.92975, saving model to mnist_conv_best.h5\n",
            "Epoch 16/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2375 - accuracy: 0.9334 - val_loss: 0.2458 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.92975 to 0.93100, saving model to mnist_conv_best.h5\n",
            "Epoch 17/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2301 - accuracy: 0.9350 - val_loss: 0.2399 - val_accuracy: 0.9344\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.93100 to 0.93442, saving model to mnist_conv_best.h5\n",
            "Epoch 18/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2215 - accuracy: 0.9379 - val_loss: 0.2344 - val_accuracy: 0.9367\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.93442 to 0.93675, saving model to mnist_conv_best.h5\n",
            "Epoch 19/10000\n",
            "188/188 [==============================] - 23s 123ms/step - loss: 0.2137 - accuracy: 0.9401 - val_loss: 0.2247 - val_accuracy: 0.9396\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.93675 to 0.93958, saving model to mnist_conv_best.h5\n",
            "Epoch 20/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.2056 - accuracy: 0.9424 - val_loss: 0.2203 - val_accuracy: 0.9398\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.93958 to 0.93975, saving model to mnist_conv_best.h5\n",
            "Epoch 21/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1982 - accuracy: 0.9447 - val_loss: 0.2142 - val_accuracy: 0.9418\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.93975 to 0.94183, saving model to mnist_conv_best.h5\n",
            "Epoch 22/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.1912 - accuracy: 0.9472 - val_loss: 0.2074 - val_accuracy: 0.9448\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.94183 to 0.94475, saving model to mnist_conv_best.h5\n",
            "Epoch 23/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1838 - accuracy: 0.9491 - val_loss: 0.2011 - val_accuracy: 0.9449\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.94475 to 0.94492, saving model to mnist_conv_best.h5\n",
            "Epoch 24/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1776 - accuracy: 0.9504 - val_loss: 0.1932 - val_accuracy: 0.9468\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.94492 to 0.94683, saving model to mnist_conv_best.h5\n",
            "Epoch 25/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1712 - accuracy: 0.9526 - val_loss: 0.1814 - val_accuracy: 0.9516\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.94683 to 0.95158, saving model to mnist_conv_best.h5\n",
            "Epoch 26/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1650 - accuracy: 0.9544 - val_loss: 0.1761 - val_accuracy: 0.9524\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.95158 to 0.95242, saving model to mnist_conv_best.h5\n",
            "Epoch 27/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1598 - accuracy: 0.9557 - val_loss: 0.1733 - val_accuracy: 0.9535\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.95242 to 0.95350, saving model to mnist_conv_best.h5\n",
            "Epoch 28/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1551 - accuracy: 0.9569 - val_loss: 0.1683 - val_accuracy: 0.9550\n",
            "\n",
            "Epoch 00028: val_accuracy improved from 0.95350 to 0.95500, saving model to mnist_conv_best.h5\n",
            "Epoch 29/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1498 - accuracy: 0.9587 - val_loss: 0.1639 - val_accuracy: 0.9562\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.95500 to 0.95617, saving model to mnist_conv_best.h5\n",
            "Epoch 30/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1455 - accuracy: 0.9595 - val_loss: 0.1587 - val_accuracy: 0.9557\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.95617\n",
            "Epoch 31/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1412 - accuracy: 0.9609 - val_loss: 0.1551 - val_accuracy: 0.9585\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.95617 to 0.95850, saving model to mnist_conv_best.h5\n",
            "Epoch 32/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.1374 - accuracy: 0.9619 - val_loss: 0.1512 - val_accuracy: 0.9590\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.95850 to 0.95900, saving model to mnist_conv_best.h5\n",
            "Epoch 33/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1336 - accuracy: 0.9629 - val_loss: 0.1477 - val_accuracy: 0.9595\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.95900 to 0.95950, saving model to mnist_conv_best.h5\n",
            "Epoch 34/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1300 - accuracy: 0.9643 - val_loss: 0.1444 - val_accuracy: 0.9607\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.95950 to 0.96075, saving model to mnist_conv_best.h5\n",
            "Epoch 35/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.1269 - accuracy: 0.9647 - val_loss: 0.1413 - val_accuracy: 0.9613\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.96075 to 0.96133, saving model to mnist_conv_best.h5\n",
            "Epoch 36/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1237 - accuracy: 0.9661 - val_loss: 0.1378 - val_accuracy: 0.9631\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.96133 to 0.96308, saving model to mnist_conv_best.h5\n",
            "Epoch 37/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1206 - accuracy: 0.9669 - val_loss: 0.1380 - val_accuracy: 0.9624\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.96308\n",
            "Epoch 38/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.1183 - accuracy: 0.9676 - val_loss: 0.1325 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.96308 to 0.96433, saving model to mnist_conv_best.h5\n",
            "Epoch 39/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1156 - accuracy: 0.9686 - val_loss: 0.1306 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.96433\n",
            "Epoch 40/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1131 - accuracy: 0.9693 - val_loss: 0.1298 - val_accuracy: 0.9643\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.96433\n",
            "Epoch 41/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1111 - accuracy: 0.9695 - val_loss: 0.1264 - val_accuracy: 0.9656\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.96433 to 0.96558, saving model to mnist_conv_best.h5\n",
            "Epoch 42/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1089 - accuracy: 0.9705 - val_loss: 0.1279 - val_accuracy: 0.9647\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.96558\n",
            "Epoch 43/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.1067 - accuracy: 0.9707 - val_loss: 0.1231 - val_accuracy: 0.9670\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.96558 to 0.96700, saving model to mnist_conv_best.h5\n",
            "Epoch 44/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1047 - accuracy: 0.9722 - val_loss: 0.1222 - val_accuracy: 0.9671\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.96700 to 0.96708, saving model to mnist_conv_best.h5\n",
            "Epoch 45/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1027 - accuracy: 0.9720 - val_loss: 0.1209 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.96708\n",
            "Epoch 46/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.1013 - accuracy: 0.9726 - val_loss: 0.1204 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.96708\n",
            "Epoch 47/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0993 - accuracy: 0.9731 - val_loss: 0.1172 - val_accuracy: 0.9682\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.96708 to 0.96817, saving model to mnist_conv_best.h5\n",
            "Epoch 48/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0977 - accuracy: 0.9737 - val_loss: 0.1147 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.96817 to 0.96875, saving model to mnist_conv_best.h5\n",
            "Epoch 49/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0961 - accuracy: 0.9741 - val_loss: 0.1134 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.96875\n",
            "Epoch 50/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0950 - accuracy: 0.9742 - val_loss: 0.1120 - val_accuracy: 0.9691\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.96875 to 0.96908, saving model to mnist_conv_best.h5\n",
            "Epoch 51/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0936 - accuracy: 0.9749 - val_loss: 0.1127 - val_accuracy: 0.9685\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.96908\n",
            "Epoch 52/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0919 - accuracy: 0.9753 - val_loss: 0.1114 - val_accuracy: 0.9701\n",
            "\n",
            "Epoch 00052: val_accuracy improved from 0.96908 to 0.97008, saving model to mnist_conv_best.h5\n",
            "Epoch 53/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0907 - accuracy: 0.9758 - val_loss: 0.1086 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.97008 to 0.97017, saving model to mnist_conv_best.h5\n",
            "Epoch 54/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0895 - accuracy: 0.9761 - val_loss: 0.1068 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00054: val_accuracy improved from 0.97017 to 0.97033, saving model to mnist_conv_best.h5\n",
            "Epoch 55/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0883 - accuracy: 0.9764 - val_loss: 0.1062 - val_accuracy: 0.9709\n",
            "\n",
            "Epoch 00055: val_accuracy improved from 0.97033 to 0.97092, saving model to mnist_conv_best.h5\n",
            "Epoch 56/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0872 - accuracy: 0.9765 - val_loss: 0.1068 - val_accuracy: 0.9701\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.97092\n",
            "Epoch 57/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0860 - accuracy: 0.9768 - val_loss: 0.1041 - val_accuracy: 0.9713\n",
            "\n",
            "Epoch 00057: val_accuracy improved from 0.97092 to 0.97133, saving model to mnist_conv_best.h5\n",
            "Epoch 58/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0849 - accuracy: 0.9769 - val_loss: 0.1042 - val_accuracy: 0.9708\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.97133\n",
            "Epoch 59/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0838 - accuracy: 0.9775 - val_loss: 0.1020 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00059: val_accuracy improved from 0.97133 to 0.97233, saving model to mnist_conv_best.h5\n",
            "Epoch 60/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0828 - accuracy: 0.9779 - val_loss: 0.1023 - val_accuracy: 0.9720\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.97233\n",
            "Epoch 61/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0821 - accuracy: 0.9779 - val_loss: 0.1008 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.97233\n",
            "Epoch 62/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0808 - accuracy: 0.9784 - val_loss: 0.0997 - val_accuracy: 0.9726\n",
            "\n",
            "Epoch 00062: val_accuracy improved from 0.97233 to 0.97258, saving model to mnist_conv_best.h5\n",
            "Epoch 63/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0801 - accuracy: 0.9786 - val_loss: 0.0988 - val_accuracy: 0.9719\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.97258\n",
            "Epoch 64/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0793 - accuracy: 0.9786 - val_loss: 0.0983 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00064: val_accuracy improved from 0.97258 to 0.97333, saving model to mnist_conv_best.h5\n",
            "Epoch 65/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0785 - accuracy: 0.9790 - val_loss: 0.0983 - val_accuracy: 0.9728\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.97333\n",
            "Epoch 66/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0776 - accuracy: 0.9793 - val_loss: 0.0980 - val_accuracy: 0.9721\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.97333\n",
            "Epoch 67/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0766 - accuracy: 0.9790 - val_loss: 0.0981 - val_accuracy: 0.9722\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.97333\n",
            "Epoch 68/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0762 - accuracy: 0.9794 - val_loss: 0.0967 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.97333\n",
            "Epoch 69/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0753 - accuracy: 0.9800 - val_loss: 0.0944 - val_accuracy: 0.9740\n",
            "\n",
            "Epoch 00069: val_accuracy improved from 0.97333 to 0.97400, saving model to mnist_conv_best.h5\n",
            "Epoch 70/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0746 - accuracy: 0.9803 - val_loss: 0.0967 - val_accuracy: 0.9725\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.97400\n",
            "Epoch 71/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0740 - accuracy: 0.9803 - val_loss: 0.0945 - val_accuracy: 0.9728\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.97400\n",
            "Epoch 72/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0732 - accuracy: 0.9803 - val_loss: 0.0931 - val_accuracy: 0.9750\n",
            "\n",
            "Epoch 00072: val_accuracy improved from 0.97400 to 0.97500, saving model to mnist_conv_best.h5\n",
            "Epoch 73/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0725 - accuracy: 0.9804 - val_loss: 0.0922 - val_accuracy: 0.9743\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.97500\n",
            "Epoch 74/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0718 - accuracy: 0.9807 - val_loss: 0.0933 - val_accuracy: 0.9745\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.97500\n",
            "Epoch 75/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0713 - accuracy: 0.9813 - val_loss: 0.0921 - val_accuracy: 0.9747\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.97500\n",
            "Epoch 76/10000\n",
            "188/188 [==============================] - 23s 124ms/step - loss: 0.0705 - accuracy: 0.9809 - val_loss: 0.0909 - val_accuracy: 0.9758\n",
            "\n",
            "Epoch 00076: val_accuracy improved from 0.97500 to 0.97575, saving model to mnist_conv_best.h5\n",
            "Epoch 77/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0700 - accuracy: 0.9811 - val_loss: 0.0899 - val_accuracy: 0.9753\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.97575\n",
            "Epoch 78/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0693 - accuracy: 0.9814 - val_loss: 0.0899 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.97575\n",
            "Epoch 79/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0688 - accuracy: 0.9816 - val_loss: 0.0904 - val_accuracy: 0.9758\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.97575\n",
            "Epoch 80/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0682 - accuracy: 0.9820 - val_loss: 0.0893 - val_accuracy: 0.9752\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.97575\n",
            "Epoch 81/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0676 - accuracy: 0.9818 - val_loss: 0.0898 - val_accuracy: 0.9739\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.97575\n",
            "Epoch 82/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0672 - accuracy: 0.9820 - val_loss: 0.0900 - val_accuracy: 0.9750\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.97575\n",
            "Epoch 83/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0664 - accuracy: 0.9822 - val_loss: 0.0886 - val_accuracy: 0.9748\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.97575\n",
            "Epoch 84/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0660 - accuracy: 0.9825 - val_loss: 0.0882 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.97575\n",
            "Epoch 85/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0657 - accuracy: 0.9828 - val_loss: 0.0879 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.97575\n",
            "Epoch 86/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0651 - accuracy: 0.9829 - val_loss: 0.0876 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00086: val_accuracy improved from 0.97575 to 0.97625, saving model to mnist_conv_best.h5\n",
            "Epoch 87/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0646 - accuracy: 0.9822 - val_loss: 0.0865 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.97625\n",
            "Epoch 88/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0641 - accuracy: 0.9832 - val_loss: 0.0859 - val_accuracy: 0.9760\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.97625\n",
            "Epoch 89/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0851 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00089: val_accuracy improved from 0.97625 to 0.97658, saving model to mnist_conv_best.h5\n",
            "Epoch 90/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0631 - accuracy: 0.9835 - val_loss: 0.0856 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.97658\n",
            "Epoch 91/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0626 - accuracy: 0.9830 - val_loss: 0.0859 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.97658\n",
            "Epoch 92/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0623 - accuracy: 0.9833 - val_loss: 0.0859 - val_accuracy: 0.9757\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.97658\n",
            "Epoch 93/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0619 - accuracy: 0.9835 - val_loss: 0.0843 - val_accuracy: 0.9765\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.97658\n",
            "Epoch 94/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0613 - accuracy: 0.9837 - val_loss: 0.0848 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.97658\n",
            "Epoch 95/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0609 - accuracy: 0.9837 - val_loss: 0.0837 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.97658\n",
            "Epoch 96/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0605 - accuracy: 0.9840 - val_loss: 0.0835 - val_accuracy: 0.9771\n",
            "\n",
            "Epoch 00096: val_accuracy improved from 0.97658 to 0.97708, saving model to mnist_conv_best.h5\n",
            "Epoch 97/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0601 - accuracy: 0.9837 - val_loss: 0.0829 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.97708\n",
            "Epoch 98/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0596 - accuracy: 0.9843 - val_loss: 0.0833 - val_accuracy: 0.9760\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.97708\n",
            "Epoch 99/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0595 - accuracy: 0.9841 - val_loss: 0.0849 - val_accuracy: 0.9760\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.97708\n",
            "Epoch 100/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0589 - accuracy: 0.9844 - val_loss: 0.0830 - val_accuracy: 0.9764\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.97708\n",
            "Epoch 101/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0586 - accuracy: 0.9842 - val_loss: 0.0820 - val_accuracy: 0.9764\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.97708\n",
            "Epoch 102/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0582 - accuracy: 0.9847 - val_loss: 0.0821 - val_accuracy: 0.9762\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.97708\n",
            "Epoch 103/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0578 - accuracy: 0.9846 - val_loss: 0.0814 - val_accuracy: 0.9770\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.97708\n",
            "Epoch 104/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0576 - accuracy: 0.9848 - val_loss: 0.0815 - val_accuracy: 0.9765\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.97708\n",
            "Epoch 105/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0570 - accuracy: 0.9849 - val_loss: 0.0818 - val_accuracy: 0.9770\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.97708\n",
            "Epoch 106/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0568 - accuracy: 0.9846 - val_loss: 0.0808 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00106: val_accuracy improved from 0.97708 to 0.97717, saving model to mnist_conv_best.h5\n",
            "Epoch 107/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0565 - accuracy: 0.9850 - val_loss: 0.0811 - val_accuracy: 0.9771\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.97717\n",
            "Epoch 108/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0560 - accuracy: 0.9853 - val_loss: 0.0801 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.97717\n",
            "Epoch 109/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0555 - accuracy: 0.9850 - val_loss: 0.0797 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.97717\n",
            "Epoch 110/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0554 - accuracy: 0.9850 - val_loss: 0.0799 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00110: val_accuracy improved from 0.97717 to 0.97750, saving model to mnist_conv_best.h5\n",
            "Epoch 111/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0551 - accuracy: 0.9851 - val_loss: 0.0807 - val_accuracy: 0.9767\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.97750\n",
            "Epoch 112/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0546 - accuracy: 0.9854 - val_loss: 0.0794 - val_accuracy: 0.9770\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.97750\n",
            "Epoch 113/10000\n",
            "188/188 [==============================] - 23s 125ms/step - loss: 0.0545 - accuracy: 0.9854 - val_loss: 0.0791 - val_accuracy: 0.9778\n",
            "\n",
            "Epoch 00113: val_accuracy improved from 0.97750 to 0.97783, saving model to mnist_conv_best.h5\n",
            "Epoch 114/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0542 - accuracy: 0.9856 - val_loss: 0.0787 - val_accuracy: 0.9774\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.97783\n",
            "Epoch 115/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0537 - accuracy: 0.9858 - val_loss: 0.0796 - val_accuracy: 0.9774\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.97783\n",
            "Epoch 116/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0535 - accuracy: 0.9858 - val_loss: 0.0784 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.97783\n",
            "Epoch 117/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0533 - accuracy: 0.9858 - val_loss: 0.0788 - val_accuracy: 0.9777\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.97783\n",
            "Epoch 118/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0529 - accuracy: 0.9856 - val_loss: 0.0795 - val_accuracy: 0.9767\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.97783\n",
            "Epoch 119/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0528 - accuracy: 0.9858 - val_loss: 0.0796 - val_accuracy: 0.9771\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.97783\n",
            "Epoch 120/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0524 - accuracy: 0.9860 - val_loss: 0.0781 - val_accuracy: 0.9768\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.97783\n",
            "Epoch 121/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0519 - accuracy: 0.9859 - val_loss: 0.0775 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.97783\n",
            "Epoch 122/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0518 - accuracy: 0.9861 - val_loss: 0.0782 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.97783\n",
            "Epoch 123/10000\n",
            "188/188 [==============================] - 24s 125ms/step - loss: 0.0514 - accuracy: 0.9861 - val_loss: 0.0769 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00123: val_accuracy improved from 0.97783 to 0.97817, saving model to mnist_conv_best.h5\n",
            "Epoch 124/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0511 - accuracy: 0.9864 - val_loss: 0.0768 - val_accuracy: 0.9784\n",
            "\n",
            "Epoch 00124: val_accuracy improved from 0.97817 to 0.97842, saving model to mnist_conv_best.h5\n",
            "Epoch 125/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0508 - accuracy: 0.9861 - val_loss: 0.0782 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.97842\n",
            "Epoch 126/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0505 - accuracy: 0.9861 - val_loss: 0.0770 - val_accuracy: 0.9778\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.97842\n",
            "Epoch 127/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0503 - accuracy: 0.9864 - val_loss: 0.0801 - val_accuracy: 0.9772\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.97842\n",
            "Epoch 128/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0500 - accuracy: 0.9861 - val_loss: 0.0774 - val_accuracy: 0.9770\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.97842\n",
            "Epoch 129/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0498 - accuracy: 0.9862 - val_loss: 0.0770 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.97842\n",
            "Epoch 130/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0497 - accuracy: 0.9871 - val_loss: 0.0772 - val_accuracy: 0.9778\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.97842\n",
            "Epoch 131/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0492 - accuracy: 0.9868 - val_loss: 0.0755 - val_accuracy: 0.9782\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.97842\n",
            "Epoch 132/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0490 - accuracy: 0.9868 - val_loss: 0.0763 - val_accuracy: 0.9771\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.97842\n",
            "Epoch 133/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0489 - accuracy: 0.9868 - val_loss: 0.0754 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.97842\n",
            "Epoch 134/10000\n",
            "188/188 [==============================] - 24s 126ms/step - loss: 0.0485 - accuracy: 0.9868 - val_loss: 0.0764 - val_accuracy: 0.9776\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.97842\n",
            "Epoch 00134: early stopping\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0554 - accuracy: 0.9850\n",
            "Accuracy for the training set: 0.9849833250045776\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0600 - accuracy: 0.9816\n",
            "Accuracy for the testing set: 0.9815999865531921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3nPVXnUAGdL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr9kcn-tU2K6"
      },
      "source": [
        "##Number of Filters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfem_6juU2LB",
        "outputId": "7cc3afea-9890-4a51-ca5a-7dbcaf499f82"
      },
      "source": [
        "Amountfilters = [2,4,8,16,32,128,248,506,784]\n",
        "acc_values = [None] * len(Amountfilters)\n",
        "for filters in range(len(Amountfilters)-1):\n",
        "  mnist_conv_model = tf.keras.Sequential(name='mnist_cnn')\n",
        "  mnist_conv_model.add(tf.keras.layers.Input(mnist_info.features['image'].shape))\n",
        "  mnist_conv_model.add(tf.keras.layers.Conv2D(filters=Amountfilters[filters], kernel_size=4, activation='relu', padding='same', name='convolution'))\n",
        "  mnist_conv_model.add(tf.keras.layers.Flatten(name='flatten'))\n",
        "  mnist_conv_model.add(tf.keras.layers.Dense(mnist_info.features['label'].num_classes, activation='softmax', name='output'))\n",
        "  mnist_conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "  mnist_conv_model.summary()\n",
        "\n",
        "  earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
        "  checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_conv_best.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "  mnist_conv_model_train = mnist_conv_model.fit(mnist_x, mnist_y, validation_split=0.2, callbacks=[earlystop,checkpoint], epochs=10000, batch_size=256)\n",
        "\n",
        "  mnist_conv_model.load_weights('mnist_conv_best.h5')\n",
        "  loss, acc = mnist_conv_model.evaluate(mnist_x, mnist_y)\n",
        "  print('Accuracy for the training set: {}'.format(acc))\n",
        "  loss, acc = mnist_conv_model.evaluate(mnist_test_x, mnist_test_y)\n",
        "  print('Accuracy for the testing set: {}'.format(acc))\n",
        "  fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
        "\n",
        "  acc_values[filters]=acc\n",
        "\n",
        "  loss_ax.set_title('Loss')\n",
        "  loss_ax.plot(mnist_conv_model_train.history['loss'], '-r', label='Train')\n",
        "  loss_ax.plot(mnist_conv_model_train.history['val_loss'], '-g', label='Validation')\n",
        "\n",
        "  acc_ax.set_title('Accuracy')\n",
        "  acc_ax.plot(mnist_conv_model_train.history['accuracy'], '-r', label='Train')\n",
        "  acc_ax.plot(mnist_conv_model_train.history['val_accuracy'], '-g', label='Validation')\n",
        "\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "plt.plot(Amountfilters,acc_values) # plotting by columns\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "convolution (Conv2D)         (None, 28, 28, 2)         34        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1568)              0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                15690     \n",
            "=================================================================\n",
            "Total params: 15,724\n",
            "Trainable params: 15,724\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "188/188 [==============================] - 10s 50ms/step - loss: 1.3883 - accuracy: 0.5885 - val_loss: 0.5843 - val_accuracy: 0.8358\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.83583, saving model to mnist_conv_best.h5\n",
            "Epoch 2/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.4721 - accuracy: 0.8658 - val_loss: 0.4229 - val_accuracy: 0.8787\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.83583 to 0.87867, saving model to mnist_conv_best.h5\n",
            "Epoch 3/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3926 - accuracy: 0.8880 - val_loss: 0.3854 - val_accuracy: 0.8919\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.87867 to 0.89192, saving model to mnist_conv_best.h5\n",
            "Epoch 4/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3645 - accuracy: 0.8965 - val_loss: 0.3636 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89192 to 0.89633, saving model to mnist_conv_best.h5\n",
            "Epoch 5/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3485 - accuracy: 0.9005 - val_loss: 0.3550 - val_accuracy: 0.9008\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.89633 to 0.90083, saving model to mnist_conv_best.h5\n",
            "Epoch 6/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3386 - accuracy: 0.9029 - val_loss: 0.3465 - val_accuracy: 0.9019\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90083 to 0.90192, saving model to mnist_conv_best.h5\n",
            "Epoch 7/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3306 - accuracy: 0.9062 - val_loss: 0.3451 - val_accuracy: 0.9040\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90192 to 0.90400, saving model to mnist_conv_best.h5\n",
            "Epoch 8/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3250 - accuracy: 0.9072 - val_loss: 0.3354 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90400 to 0.90758, saving model to mnist_conv_best.h5\n",
            "Epoch 9/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3196 - accuracy: 0.9086 - val_loss: 0.3265 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90758 to 0.91033, saving model to mnist_conv_best.h5\n",
            "Epoch 10/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3149 - accuracy: 0.9099 - val_loss: 0.3253 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.91033\n",
            "Epoch 11/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3119 - accuracy: 0.9108 - val_loss: 0.3286 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.91033\n",
            "Epoch 12/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3083 - accuracy: 0.9130 - val_loss: 0.3217 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.91033 to 0.91133, saving model to mnist_conv_best.h5\n",
            "Epoch 13/10000\n",
            "188/188 [==============================] - 9s 50ms/step - loss: 0.3057 - accuracy: 0.9132 - val_loss: 0.3225 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91133\n",
            "Epoch 14/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3027 - accuracy: 0.9146 - val_loss: 0.3139 - val_accuracy: 0.9147\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.91133 to 0.91475, saving model to mnist_conv_best.h5\n",
            "Epoch 15/10000\n",
            "188/188 [==============================] - 9s 50ms/step - loss: 0.3001 - accuracy: 0.9150 - val_loss: 0.3152 - val_accuracy: 0.9132\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91475\n",
            "Epoch 16/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2981 - accuracy: 0.9154 - val_loss: 0.3161 - val_accuracy: 0.9133\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91475\n",
            "Epoch 17/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2956 - accuracy: 0.9162 - val_loss: 0.3091 - val_accuracy: 0.9152\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.91475 to 0.91517, saving model to mnist_conv_best.h5\n",
            "Epoch 18/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2943 - accuracy: 0.9178 - val_loss: 0.3093 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91517\n",
            "Epoch 19/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2916 - accuracy: 0.9175 - val_loss: 0.3066 - val_accuracy: 0.9162\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.91517 to 0.91625, saving model to mnist_conv_best.h5\n",
            "Epoch 20/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2910 - accuracy: 0.9179 - val_loss: 0.3081 - val_accuracy: 0.9147\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91625\n",
            "Epoch 21/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2885 - accuracy: 0.9186 - val_loss: 0.3051 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.91625 to 0.91833, saving model to mnist_conv_best.h5\n",
            "Epoch 22/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2873 - accuracy: 0.9191 - val_loss: 0.3067 - val_accuracy: 0.9158\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91833\n",
            "Epoch 23/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2860 - accuracy: 0.9193 - val_loss: 0.3070 - val_accuracy: 0.9126\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91833\n",
            "Epoch 24/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2846 - accuracy: 0.9201 - val_loss: 0.3156 - val_accuracy: 0.9122\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91833\n",
            "Epoch 25/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2837 - accuracy: 0.9204 - val_loss: 0.3053 - val_accuracy: 0.9143\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91833\n",
            "Epoch 26/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2821 - accuracy: 0.9201 - val_loss: 0.3002 - val_accuracy: 0.9189\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.91833 to 0.91892, saving model to mnist_conv_best.h5\n",
            "Epoch 27/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2814 - accuracy: 0.9202 - val_loss: 0.2950 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.91892 to 0.92025, saving model to mnist_conv_best.h5\n",
            "Epoch 28/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2802 - accuracy: 0.9208 - val_loss: 0.2953 - val_accuracy: 0.9188\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.92025\n",
            "Epoch 29/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2788 - accuracy: 0.9212 - val_loss: 0.3011 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.92025\n",
            "Epoch 30/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2783 - accuracy: 0.9215 - val_loss: 0.2961 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.92025\n",
            "Epoch 31/10000\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.2774 - accuracy: 0.9227 - val_loss: 0.3114 - val_accuracy: 0.9129\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.92025\n",
            "Epoch 32/10000\n",
            "165/188 [=========================>....] - ETA: 1s - loss: 0.2775 - accuracy: 0.9211"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E18W8f2JU2LF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}